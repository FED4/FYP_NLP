{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd9f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_db = pd.read_csv('data/Sample_db_crisps.csv')\n",
    "df_search = pd.read_csv('data/nutrition_table_crisps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ab73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_search = pd.read_csv('data/Sample_db_crisps.csv')\n",
    "df_db = pd.read_csv('data/nutrition_table_crisps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ba0b5926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_search = pd.read_csv('data/Sample_db.csv')\n",
    "df_db = pd.read_csv('data/nutrition_table_crisps.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b97361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_search = pd.read_csv('data/Sample_db.csv')\n",
    "df_db = pd.read_csv('data/nutrition_table.csv', encoding= 'unicode_escape' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dcb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1c50557d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 768)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write some lines to encode (sentences 0 and 2 are both ideltical):\n",
    "sen = [\n",
    "    \"Three years later, the coffin was still full of Jello.\",\n",
    "    \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "    \"The person box was packed with jelly many dozens of months later.\",\n",
    "    \"He found a leprechaun in his walnut shell.\"\n",
    "]\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "#Encoding:\n",
    "sen_embeddings = model.encode(df_search['Product Name'].tolist())\n",
    "sen_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "798ac995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "#nitialize our model and tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "def Bert_Embedding(df):\n",
    "    # initialize dictionary: stores tokenized sentences\n",
    "    token = {'input_ids': [], 'attention_mask': []}\n",
    "    for index, row in df.iterrows():\n",
    "        # encode each sentence, append to dictionary\n",
    "        sentence = row['Product Name']\n",
    "        new_token = tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                           truncation=True, padding='max_length',\n",
    "                                           return_tensors='pt')\n",
    "        token['input_ids'].append(new_token['input_ids'][0])\n",
    "        token['attention_mask'].append(new_token['attention_mask'][0])\n",
    "    # reformat list of tensors to single tensor\n",
    "    token['input_ids'] = torch.stack(token['input_ids'])\n",
    "    token['attention_mask'] = torch.stack(token['attention_mask'])\n",
    "\n",
    "    #Process tokens through model:\n",
    "    output = model(**token)\n",
    "\n",
    "    #The dense vector representations of text are contained within the outputs 'last_hidden_state' tensor\n",
    "    embeddings = output.last_hidden_state\n",
    "\n",
    "    # To perform this operation, we first resize our attention_mask tensor:\n",
    "    att_mask = token['attention_mask']\n",
    "    mask = att_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    mask_embeddings = embeddings * mask\n",
    "\n",
    "    #Then we sum the remained of the embeddings along axis 1:\n",
    "    summed = torch.sum(mask_embeddings, 1)\n",
    "\n",
    "    summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "    print(summed_mask.shape)\n",
    "    mean_pooled = summed / summed_mask\n",
    "    return mean_pooled.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38340b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# Need to load the large model to get the vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def spacy_embedding(df):\n",
    "    with nlp.disable_pipes():\n",
    "        doc_vectors = np.array([nlp(text).vector for text in df['Product Name']])\n",
    "    return doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cda5dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#row_index = ind[0][0]\n",
    "#df_db['Energy'] = df_db['Energy'].astype(float)\n",
    "#db_tree_row = df_db.iloc[row_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00265fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pringles Original Sharing Crisps 200g\n",
      "Pringles Sour Cream & Onion Sharing Crisps 200g\n",
      "Pringles Salt & Vinegar Sharing Crisps 200g\n",
      "Walkers Ready Salted Multipack Crisps 6x25g\n",
      "Walkers Cheese & Onion Multipack Crisps 6x25g\n",
      "Pom-Bear Original Multipack Crisps 6x13g\n",
      "Walkers Classic Variety Multipack Crisps 12x25g\n",
      "Kettle Chips Lightly Salted Sharing Crisps 150g\n",
      "Sainsbury's Ready Salted Crisps 6x25g\n",
      "Sainsbury's Gourmet Sea Salt Crisps, Taste the Difference 150g\n",
      "Doritos Cool Original Sharing Tortilla Chips Crisps 180g\n",
      "Sensations Thai Sweet Chilli Sharing Crisps 150g\n",
      "Walkers Prawn Cocktail Multipack Crisps 6x25g\n",
      "Hula Hoops Original Potato Ring Crisps 6x24g\n",
      "Walkers Salt & Vinegar Multipack Crisps 6x25g\n",
      "Sainsbury's Sea Salt & Cider Vinegar Crisps, Taste the Difference 150g\n",
      "Sainsbury's Cheddar & Spring Onion Crisps, Taste the Difference 150g\n",
      "Walkers Salt & Shake Multipack Crisps 6x24g\n",
      "Hula Hoops BBQ Beef Potato Ring Crisps 6x24g\n",
      "Walkers Classic Variety Multipack Crisps 22x25g\n",
      "Kettle Chips Sea Salt & Balsamic Vinegar of Modena Sharing Crisps 150g\n",
      "McCoy's Flame Grilled Steak Variety Pack Crisps 6x25g\n",
      "Sainsbury's Cheese & Onion Crisps 6x25g\n",
      "Hula Hoops Variety Crisps 12x24g\n",
      "McCoy's Classic Variety Pack Crisps 6x25g\n",
      "Walkers Classic Variety Multipack Crisps 6x25g\n",
      "Walkers Ready Salted Crisps 12x25g\n",
      "Doritos Chilli Heatwave Sharing Tortilla Chips Crisps 180g\n",
      "Walkers Smoky Bacon Multipack Crisps 6x25g\n",
      "Kettle Chips Sea Salt & Crushed Black Peppercorns Sharing Crisps 150g\n",
      "Walkers Roast Chicken Multipack Crisps 6x25g\n",
      "Kettle Chips Lightly Salted Multipack Crisps 5x30g\n",
      "Popchips Barbeque Multipack Crisps 5pk\n",
      "McCoy's Salt & Vinegar Variety Pack Crisps 6x25g\n",
      "Sainsbury's Classic Variety Assorted Crisps 22x25g\n",
      "Pringles Texas BBQ Sauce Flavour Sharing Crisps 200g\n",
      "Walkers Less Salt Lightly Salted Multipack Crisps 6x25g\n",
      "Kettle Chips Sweet Chilli & Sour Cream Sharing Crisps 150g\n",
      "Kettle Chips Mature Cheddar & Red Onion Sharing Crisps 150g\n",
      "Walkers Marmite Multipack Crisps 6x25g\n",
      "Jacob's Crinklys Cheese & Onion Crisps 6x25g\n",
      "Tyrrells Lightly Sea Salted Sharing Crisps 150g\n",
      "Walkers Sensations Thai Sweet Chilli Multipack Crisps 5x25g\n",
      "Sainsbury's Assorted Crisps 6x25g\n",
      "Sainsbury's Salt & Vinegar Crisps 6x25g\n",
      "Jacob's Twiglets Crisps 6x24g\n",
      "Popchips Barbeque Sharing Crisps 85g\n",
      "Sainsbury's Ready Salted Crisps 12x25g\n",
      "Walkers Meaty Variety Multipack Crisps 22x25g\n",
      "Sensations Roast Chicken & Thyme Sharing Crisps 150g\n",
      "Walkers Less Salt Mild Cheese & Onion Multipack Crisps 6x25g\n",
      "Popchips Sea Salt & Vinegar Sharing Crisps 85g\n",
      "Hula Hoops Original Crisps 12x24g\n",
      "Doritos Tangy Cheese Sharing Tortilla Chips Crisps 180g\n",
      "Jacob's Twiglets Crisps 150g\n",
      "Sainsbury's Classic Variety Assorted Crisps 12x25g\n",
      "Popchips Sour Cream & Onion Multipack Crisps 5pk\n",
      "Pringles Paprika Sharing Crisps 200g\n",
      "Tyrrells Lightly Sea Salted Crisps 6x25g\n",
      "Jacob's Cracker Crisps Salt & Vinegar 150g\n",
      "Kellogg's Crunchy Nut Not So Nutty! Granola 600g\n",
      "Kellogg's Fruit Winders Strawberry Snack 5x17g\n",
      "Kellogg's Fruit Winders Doubles Apple & Strawberry Rolls 5x17g\n",
      "Sainsbury's Thai Sweet Chilli Crisps, Taste The Difference 150g\n",
      "Hula Hoops Variety Pack Potato Ring Crisps 6x24g\n",
      "Hula Hoops Puft Salted Crisps 6x15g\n",
      "Sainsbury's Vegetable Crisps, Taste the Difference 100g\n",
      "Kettle Chips Sour Cream & Sweet Onion Sharing Crisps 150g\n",
      "Pringles Prawn Cocktail Flavour Sharing Crisps 200g\n",
      "Tyrrells Cider Vinegar & Salt Sharing Crisps 150g\n",
      "Pringles Cheese & Onion Sharing Crisps 200g\n",
      "Tyrrells Mixed Root Vegetable Sharing Crisps 125g\n",
      "Walkers Less Salt A Dash of Salt & Vinegar Multipack Crisps 6x25g\n",
      "Sensations Balsamic Vinegar & Caramelised Onion Sharing Crisps 150g\n",
      "Hula Hoops Puft Salt & Vinegar Crisps 6x15g\n",
      "Doritos Lightly Salted Sharing Tortilla Chips Crisps 270g\n",
      "Walkers Cheese & Onion Crisps 12x25g\n",
      "Sainsbury's Prawn Cocktail Crisps 6x25g\n",
      "Tyrrells Sweet Chilli & Red Pepper Sharing Crisps 150g\n",
      "Tyrrells Cheddar Cheese & Chive Sharing Crisps 150g\n",
      "Jacob's Cracker Crisps Sour Cream & Chive 150g\n",
      "Walkers Meaty Variety Crisps 12x25g\n",
      "Pringles Smokey Bacon Flavour Sharing Crisps 200g\n",
      "Popchips Sour Cream & Onion Sharing Crisps 85g\n",
      "Tyrrells Lentil Sharing Crisps Sour Cream & Onion 80g\n",
      "Doritos Hint of Lime Sharing Tortilla Chips Crisps 270g\n",
      "Tyrrells Lentil Sharing Crisps Sweet Chilli & Red Pepper 80g\n",
      "Pom-Bear Cheese & Onion Multipack Crisps 6x13g\n",
      "McCoy's Meaty Variety Pack Crisps 6x25g\n",
      "Popchips Original Chips Sharing Crisps 85g\n",
      "Pipers Anglesey Sea Salt Sharing Crisps 150g\n",
      "Kettle Chips Variety Multipack Crisps 5x30g\n",
      "Sainsbury's Ridge Classic Variety Ridge Cut Assorted Crisps 6x27g\n",
      "Sainsbury's Flame Grilled Steak Ridge Cut Crisps 6x27g\n",
      "Sainsbury's Coronation Chicken Flavour Hand Cooked Potato Crisps, Taste the Difference 150g\n",
      "McCoy's Cheddar & Onion Variety Pack Crisps 6x25g\n",
      "Walkers Sensations BBQ Beef Teriyaki Sharing Crisps 150g\n",
      "Walkers Max Strong Jalape?o & Cheese Sharing Crisps 150g\n",
      "Walkers Max Punchy Paprika Sharing Crisps 150g\n",
      "Pipers Lye Cross Cheddar & Onion Sharing Crisps 150g\n",
      "Pipers Great Berwick Longhorn Beef Crisps 150g\n",
      "Walkers Max Strong Hot Chicken Wings Sharing Crisps 150g\n",
      "Pipers Burrow Hill Cider Vinegar & Sea Salt Sharing Crisps 150g\n",
      "McCoy's Thai Sweet Chicken Multipack Crisps 6 Pack\n",
      "Doritos Flamin' Hot Tangy Cheese Sharing Tortilla Chips Crisps 180g\n",
      "Pringles Sizzl'N Extra Hot Cheese & Chilli Crisps 180g\n",
      "Pringles Sizzl'N Kickin' Sour Cream Sharing Crisps 180g\n",
      "Pipers Kirkby Malham Chorizo Sharing Crisps 150g\n",
      "Walkers Cheese & Onion Crisps 45g\n",
      "Tyrrells English Smoky Barbecue Sharing Crisps 150g\n",
      "Pipers Atlas Mountains Wild Thyme & Rosemary Sharing Crisps 150g\n",
      "Tyrrells Sea Salt & Black Pepper Sharing Crisps 150g\n",
      "Doritos Stax Ultimate Cheese Sharing Snacks Crisps 170g\n",
      "Walkers Salt & Vinegar Crisps 45g\n",
      "Pringles Sizzl'n Spicy Sweet Chilli Crisps 180g\n",
      "Doritos Stax Sour Cream & Onion Sharing Snacks Crisps 170g\n",
      "Walkers Prawn Cocktail Crisps 45g\n",
      "Pringles Sizzl'N Spicy Chorizo Flavour Sharing Crisps 180g\n",
      "Doritos Flame Grilled Steak Sharing Tortilla Chips Crisps 180g\n",
      "Walkers Max KFC Kentucky Fried Chicken Sharing Crisps 140g\n",
      "Tyrrells Veg Balsamic Vinegar & Sea Salt Sharing Crisps 125g\n",
      "Kettle Veg Chips Lightly Salted Sharing Crisps 125g\n",
      "Pringles Sour Cream & Onion Sharing Crisps 200g\n",
      "Pringles Texas BBQ Sauce Flavour Sharing Crisps 200g\n",
      "Kellogg's Crunchy Nut Not So Nutty! Granola 600g\n",
      "Kellogg's Fruit Winders Doubles Apple & Strawberry Rolls 5x17g\n",
      "Pipers Upton Cheyney Jalape?o & Dill Crisps 150g\n",
      "Walkers Roast Chicken Crisps 45g\n",
      "Walkers Smoky Bacon Crisps 45g\n",
      "Walkers Oven Baked Prawn Cocktail Flavoured Crisp Snacks 37.5g\n",
      "Walkers Max Strong Hot Chicken Wings Crisps 50g\n",
      "Jacob's Mini Cheddars Original x6 25g\n",
      "Just Snax Tortilla Chips, Basics 200g\n",
      "Walkers Quavers Cheese Multipack Snacks 6x16g\n",
      "Walkers Wotsits Really Cheesy Multipack Snacks 6x16.5g\n",
      "Walkers Baked Sea Salt Mulitpack Snacks 6x25g\n",
      "Jacob's Mini Cheddars Red Leicester x6 25g\n",
      "Smiths Frazzles Crispy Bacon Multipack Snacks 6x18g\n",
      "Jacob's Mini Cheddars Original x12 25g\n",
      "Walkers Sunbites Sweet Chilli Mulitpack Snacks 6x25g\n",
      "Walkers Quavers Cheese Multipack Snacks 12x16g\n",
      "Walkers Squares Salt & Vinegar Multipack Snacks 6x22g\n",
      "Skips Prawn Cocktail 6x13.1g\n",
      "Walkers Wotsits Really Cheesy Multipack Snacks 12x16.5g\n",
      "Sainsbury's Tortilla Chips Salted 200g\n",
      "Walkers Baked Cheese & Onion Mulitpack Snacks 6x25g\n",
      "Snack a Jacks Salt & Vinegar Multipack Rice Cakes Pack x5\n",
      "Sensations Lime & Coriander Chutney Poppadoms 82.5g\n",
      "Walkers Baked Variety Mulitpack Snacks 6x25g\n",
      "Sunbites Sour Cream Mulitpack Snacks 6x25g\n",
      "Walkers French Fries Variety Multipack Snacks 12x18g\n",
      "Eat Real Lentil Chips Sea Salt Flavour 113g\n",
      "Jacob's Mini Cheddars Red Leicester x12 300g\n",
      "Sainsbury's Mild Salsa 200g\n",
      "Sainsbury's Onion Rings 125g\n",
      "Sainsbury's Bacon Crispies 140g\n",
      "Smiths Chipsticks Salt & Vinegar Multipack Snacks 6x17g\n",
      "Walkers Squares Variety Multipack Snacks 12x22g\n",
      "Walkers Monster Munch Pickled Onion Multipack Snacks 6x20g\n",
      "Walkers Baked Salt & Vinegar Mulitpack Snacks 6x25g\n",
      "Sainsbury's Cheese Balls 140g\n",
      "Walkers Quavers Prawn Cocktail Multipack Snacks x6 16g\n",
      "Wheat Crunchies Bacon 6x23g\n",
      "Sensations Mango & Chilli Chutney Poppadoms 82.5g\n",
      "Doritos Variety Multipack Tortilla Chips 12x30g\n",
      "Sainsbury?s Salt & Vinegar Crunchy Sticks 140g\n",
      "Off the Eaten Path Balsamic Vinegar Bean Sticks 100g\n",
      "Doritos Chilli Heatwave Multipack Tortilla Chips 5pk\n",
      "Sainsbury's Onion Rings 6x19g\n",
      "Doritos Tangy Cheese Multipack Tortilla Chips 5x30g\n",
      "Sainsbury's Cheese Puffs 6x17g\n",
      "Sainsbury's Bacon Crispies 6x20g\n",
      "Walkers Monster Munch Roast Beef Multipack Snacks 6x20g\n",
      "Walkers Wotsits Giants Really Cheesy Sharing Snacks 130g\n",
      "Skips Prawn Cocktail 14x13.1g\n",
      "Sainsbury's Salt  Potato Twirls 125g\n",
      "Cheetos Puffs Flamin' Hot Multipack Snacks 6x13g\n",
      "Properchips Barbecue Lentil Chips 85g\n",
      "Walkers Monster Munch Variety Multipack Snacks 12x20g\n",
      "Sainsbury's Prawn Cocktail Shells 85g\n",
      "Mister Free'd Tortilla Chips BBQ 135g\n",
      "Eat Real Hummus, Lentil, Quinoa Chips X5 116g\n",
      "Mister Free'd Cheezie Tortilla Chips Cheese 135g\n",
      "Itsu Crispy Seaweed Thins Sea Salt 3x5g\n",
      "Sainsbury's Tortilla Chips Tangy Cheese 200g\n",
      "Pringles Sour Cream & Onion Sharing Crisps 200g\n",
      "Pringles Salt & Vinegar Sharing Crisps 200g\n",
      "Pringles Texas BBQ Sauce Flavour Sharing Crisps 200g\n",
      "Kellogg's Fruit Winders Strawberry Snack 5x17g\n",
      "Walkers Quavers Salt & Vinegar Multipack Snacks x6 16g\n",
      "Off the Eaten Path Sea Salt Bean Sticks 100g\n",
      "Snack a Jacks Sour Cream & Chive Multipack Rice Cakes\n",
      "Walkers Wotsits Giants Flamin' Hot Sharing Snacks 130g\n",
      "Sainsbury's Cheese Puffs 125g\n",
      "Sainsbury's Cheesy Curls 6x16g\n",
      "Hula Hoops Variety  Pack 18x24g\n",
      "Eat Real Quinoa Chips Sour Cream & Chives Flavour 80g\n",
      "Cheetos Puffs Cheese Multipack Snacks 6x13g\n",
      "Harvest Snaps Sour Cream & Chive Crunchy Lentil Rings 6x17g\n",
      "Sainsbury's Salted Popped Potato Chips 88g\n",
      "Walkers Monster Munch Giants Pickled Onion 85g\n",
      "Walkers Wotsits Crunchy Really Cheesy Snacks 140g\n",
      "Properchips Salt & Vinegar Lentil Chips 85g\n",
      "Walkers Monster Munch Giants Roast Beef 85g\n",
      "Sainsbury's Tortilla Chips Chilli  200g\n",
      "Snack a Jacks Sweet Chilli Multipack Rice Cakes 5pk\n",
      "Sainsbury's Lion Snacks Lightly Salted 6x15g\n",
      "Manomasa Manchego & Green Olive Sharing Tortilla Chips 160g\n",
      "Discos Variety Pack 6x25.5g\n",
      "Sch?r Gluten Free Curvies Original 170g\n",
      "Jacob's Mini Cheddars Sticks Rich & Tangy Cheddar Flavour 150g\n",
      "Harvest Snaps Thai Sweet Chilli Crispy Lentil Puffs 6x18g\n",
      "Walkers Oven Baked Prawn Cocktail Snacks 6x25g\n",
      "Manomasa Chipotle & Lime Sharing Tortilla Chips 160g\n",
      "Sainsbury's BBQ Popped Potato Chips 88g\n",
      "Kind Maple Pecan Almond Snack Bars Multipack 3x30g\n",
      "Sensations Peking Spare Rib Oriental Crackers 110g\n",
      "Walkers Wotsits Giants Prawn Cocktail Snacks 105g\n",
      "Sainsbury's Beef & Caramelised Onion, Taste the Difference\n",
      "Propercorn Salted Caramel Popcorn 90g\n",
      "Off the Eaten Path Sour Cream & Pepper Bean Sticks 100g\n",
      "Walkers Wotsits Really Cheesy Multipack Snacks 22x16.5g\n",
      "Itsu Crispy Seaweed Thins Sweet Soy & Sea Salt 3x5g\n",
      "Walkers Sensations Garlic & Herb Sharing Naan Chips 150g\n",
      "Jacob's Mini Cheddars Sticks Grilled Cheddar & Sizzling Steak Flavour 150g\n",
      "Hippeas Chickpea Puffs Sweet & Smokin 78g\n",
      "Jacob's Mini Cheddars Crunchlets Rich & Tangy Cheddar Flavour 115g\n",
      "Eat Real Veggie Straws Kale Tomato Spinach 5x20g\n",
      "Tyrrells Salt & Cider Vinegar 6x25g\n",
      "Sainsbury's Limited Summer Edition Chorizo Salsa Flavour, Taste the Difference 150g\n",
      "Hippeas Chickpea Puffs Salt & Vinegar Vibes 78g\n",
      "Off The Eaten Path Thai Chilli & Lime Rice & Pea Chips 120g\n",
      "Manomasa Serrano Chilli & Yucatan Honey Sharing Tortilla Chips 160g\n",
      "Mr Porky Hand Cooked Pork Scratchings 65g\n",
      "Manomasa Green Lemon & Pink Peppercorn Sharing Tortilla Chips 160g\n",
      "Off the Eaten Path Sea Salt Rice and Pea Chips 120g\n",
      "Sainsbury's Baked Spiced Turmeric & Coconut Flatbread Chips 150g\n",
      "Walkers Wotsits Crunchy Flamin' Hot Snacks 140g\n",
      "Jacob's Mini Cheddars Crunchlets Cheddar & Caramelised Onion Flavour 115g\n",
      "Sainsbury's Sea Salt & Rosemary Flatbread Chips 150g\n",
      "Sainsbury's Summer Edition Ham & Piccalilli Flavour, Taste the Difference 150g\n",
      "Sainsbury's Tortilla Chips Cool 200g\n",
      "Harvest Snaps Sour Cream & Chive Crunchy Lentil Rings 100g\n",
      "Sainsbury's Maple Bacon Pretzels 150g\n",
      "Doritos Stax Flaming Chicken Wings Flavour Corn Chips 170g\n",
      "Peter's Yard West Country Sour Cream & Chive Sourdough Bites 90g\n",
      "Kind Fruit & Nut Multipack 3x30g\n",
      "Penn State Roasted Chilli Sharing Pretzels 165g\n",
      "Pringles Original Sharing Crisps 200g\n",
      "Pringles Salt & Vinegar Sharing Crisps 200g\n",
      "Kellogg's Fruit Winders Strawberry Snack 5x17g\n",
      "Kellogg's Fruit Winders Doubles Apple & Strawberry Rolls 5x17g\n",
      "Koikeya Original Premium Japanese Potato Chips Wasabi Nori 100g\n",
      "Harvest Snaps Thai Sweet Chilli Crispy Lentil Puffs 100g\n",
      "Tropical Sun Cassava Chips Lightly Salted 80g\n",
      "Pretzel Pete Honey Mustard & Onion Pieces 160g\n",
      "Peter's Yard Suffolk Cyder Vinegar & Sea Salt Sourdough Bites 90g\n",
      "Sainsbury's Onion & Garlic, Be Good To Yourself 200g\n",
      "Ella's Kitchen Organic Cheese and Tomato & Basil Mini Puffs Multipack Baby Snack 10 Months 4x8g\n",
      "Tropical Sun Cassava Chips Chilli & Lime Flavour 80g\n",
      "Big Hoops Salted Grab Bag Crisps 45g\n",
      "Sensations Thai Sweet Chilli Crisps 40g\n",
      "Big Hoops BBQ Beef Grab Bag Crisps 45g\n",
      "Walkers Quavers Cheese Crisp Snacks 34g\n",
      "Nik Naks Nice'N'Spicy Grab Bag Crisps 45g\n",
      "Walkers Max Flame Grilled Steak Crisps 50g\n",
      "Doritos Cool Grab Original Tortilla Chip Crisps 48g\n",
      "Skips Prawn Cocktail Grab Bag Crisps 35g\n",
      "Walkers Max Crisps Paprika 50g\n",
      "McCoy's Thai Sweet Chicken Grab Bag Crisps 45g\n",
      "Walkers Wotsits Really Cheesy Crisp Snacks 36g\n",
      "Popchips Sour Cream & Onion Crisps 23g\n",
      "McCoy's Salt and Malt Vinegar Grab Bag Crisps 45g\n",
      "Doritos Tangy Cheese Tortilla Chip Crisps 48g\n",
      "Jacob's Mini Cheddars Original Crisps 50g\n",
      "Popchips Sea Salt Potato Crisps 23g\n",
      "Walkers Baked Cheese & Onion Crisps 37.5g\n",
      "Walkers Max Cheese & Onion Crisps 50g\n",
      "Tyrrells Mixed Root Vegetable Crisps 40g\n",
      "Walkers Baked Ready Salted Crisps 37.5g\n",
      "Tyrrells Sweet Chilli & Red Pepper Crisps 40g\n",
      "Popchips Barbeque Potato Chip Crisps 23g\n",
      "Kettle Chips Sea Salt & Balsamic Vinegar Crisps 40g\n",
      "Stamford Street Food Company Houmous 200g\n",
      "Walkers French Fries Ready Salted Multipack Snacks 6x18g\n",
      "Snack A Jacks Caramel Rice Cakes 159g\n",
      "Sainsbury's Cool Salsa Dip 300g\n",
      "Snack A Jacks Chocolate Chip Rice Cakes 180g\n",
      "Sainsbury's Salted Pretzels 150g\n",
      "Penn State Sour Cream & Chives Sharing Pretzels 175g\n",
      "Walkers Squares Variety Multipack Snacks 6x22g\n",
      "Snack a Jacks Salt & Vinegar Rice Cakes 126g\n",
      "Sainsbury's Cheese Savouries 250g\n",
      "Sainsbury's Mini Poppadoms 100g\n",
      "Sainsbury's Hot Salsa Dip 300g\n",
      "Penn State Sea Salted Sharing Pretzels 175g\n",
      "Metcalfe's Sweet & Salt Popcorn Sharing Bag 80g\n",
      "Butterkist Popcorn Toffee 6x20g\n",
      "Doritos Mild Salsa Dip 300g\n",
      "Metcalfe's Sea Salt Popcorn Sharing Bag 70g\n",
      "Sabra Houmous Extra 350g\n",
      "Sainsbury's Tortilla Chips 200g\n",
      "Sainsbury's Deliciously Free From Garlic & Coriander Naan Breads 2 x 100g\n",
      "Doritos Hot Salsa Dip 300g\n",
      "Hippeas Chickpea Puffs Sweet & Smokin 5pk\n",
      "Old El Paso Gluten Free Crunchy Nacho Chips 185g\n",
      "Sainsbury's Mini Poppadums 100g\n",
      "Snack a Jacks Cheese Rice Cakes 120g\n",
      "Sainsbury's Sour Cream & Chive Combo Snacks 150g\n",
      "Wonderful Roasted Salted XXL Pistachios 115g\n",
      "Kellogg's Fruit Winders Doubles Apple & Strawberry Rolls 5x17g\n",
      "Pringles Original Sharing Crisps 200g\n",
      "Pringles Sour Cream & Onion Sharing Crisps 200g\n",
      "Kellogg's Crunchy Nut Not So Nutty! Granola 600g\n",
      "Sainsbury's Sour Cream & Chive Dip 280g\n",
      "Sainsbury's Salt & Pepper Combo Snack Mix 150g\n",
      "Sainsbury's Free From White Tortilla Wrap x4\n",
      "Sabra Houmous with Hot Chili Pepper 200g\n",
      "Doritos Cool Sour Cream & Chives Sharing Dip 280g\n",
      "Butterkist Popcorn, Sweet Cinema Style 6x12g\n",
      "Gran Luchito Mexican Tortilla Chips Lightly Salted 170g\n",
      "Sainsbury's Deliciously Free From Lightly Salted Tortilla Chips 200g\n",
      "Sainsbury's Deliciously Free From Sweet Potato Tortilla Wraps x4\n",
      "Doritos Nacho Cheese Sharing Dip 280g\n",
      "Pringles Original 40g\n",
      "Pringles Sour Cream & Onion 40g\n",
      "Forest Feast Slow Roasted Sea Salt & Black Peppercorn Nut Mix 120g\n",
      "Sainsbury's Cheese&onion Chickpea Snacks 6pk 120g\n",
      "Boostball Chocolate Brownie Keto Snack 4x10g\n",
      "Howdah Onion Bhaji Crunchy Crescents with Zingy Spices 150g\n",
      "PROPERCHIPS Barbecue Lentil Chips 20g\n",
      "Kings Rib Eye Flavour Biltong 65g\n",
      "Kings Beef Jerky BBQ 65g\n",
      "Hippeas Salt & Vinegar Vibes Mulltipack\n",
      "Howdah Tandoor Chilli Ancient Grain Chips 130g\n",
      "Reese's Dipped Pretzels 120g\n",
      "Forest Feast Slow Roasted Heather Honey Peanuts & Cashews 120g\n",
      "Wild West Honey BBQ Beef Jerky 100g\n",
      "Mega Monster Munch Pickled Onion Snacks 40g\n",
      "Bare Fruit Snacks Banana Chips 24g\n",
      "Snack a Jacks Rice and Corn Cakes Salt & Vinegar 23g\n",
      "Lorenz Crunchips Paprika 140g\n",
      "Capsicana Tortilla Chips Lightly Salted 170g\n",
      "Walkers Squares Salt & Vinegar Snacks 27.5g\n",
      "Bare Fruit Snacks Coconut Chips 24g\n",
      "Snack a Jacks Rice Cakes Sweet Chilli 23g\n",
      "Mega Monster Munch Roast Beef Snacks 40g\n",
      "Sainsbury's Ancho Chilli Bean Dip, Taste the Difference 300g\n",
      "Hippeas Chickpea Tortilla Snacks Chilli Kicks Flavour 130g\n",
      "Hippeas Chickpea Tortilla Snacks Cheesy Nacho Vibes Flavour 130g\n",
      "Sunbites Sweet Chilli Multigrain Snacks 28g\n",
      "Walkers Baked Salt & Vinegar Snack 37.5g\n",
      "Walkers Squares Cheese & Onion Snacks % 27.5g\n",
      "Well & Truly Crunchies Sea Salt & Cider Vinegar Baked Corn Snacks 30g\n",
      "Hippeas Chickpea Tortilla Snacks Rockin' Ranch Flavour 130g\n",
      "Metcalfe's Popcorn Sea Salt 17g\n",
      "Pringles Texas BBQ Sauce Flavour Sharing Crisps 200g\n",
      "Pringles Sour Cream & Onion Sharing Crisps 200g\n",
      "Pringles Original Sharing Crisps 200g\n",
      "Kellogg's Fruit Winders Doubles Apple & Strawberry Rolls 5x17g\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Candidate Matchers\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "KD-Tree Implemention\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "'''\n",
    "\n",
    "db_embeded = Bert_Embedding(df_db)\n",
    "print(df_search)\n",
    "db_search = Bert_Embedding(df_search)\n",
    "'''\n",
    "\n",
    "\n",
    "df = df_db\n",
    "\n",
    "# initialize dictionary: stores tokenized sentences\n",
    "token = {'input_ids': [], 'attention_mask': []}\n",
    "for index, row in df.iterrows():\n",
    "    # encode each sentence, append to dictionary\n",
    "    sentence = row['Product Name']\n",
    "    print(sentence)\n",
    "    new_token = tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                       truncation=True, padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    token['input_ids'].append(new_token['input_ids'][0])\n",
    "    token['attention_mask'].append(new_token['attention_mask'][0])\n",
    "# reformat list of tensors to single tensor\n",
    "token['input_ids'] = torch.stack(token['input_ids'])\n",
    "token['attention_mask'] = torch.stack(token['attention_mask'])\n",
    "\n",
    "#Process tokens through model:\n",
    "output = model(**token)\n",
    "\n",
    "#The dense vector representations of text are contained within the outputs 'last_hidden_state' tensor\n",
    "embeddings = output.last_hidden_state\n",
    "\n",
    "# To perform this operation, we first resize our attention_mask tensor:\n",
    "att_mask = token['attention_mask']\n",
    "mask = att_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask_embeddings = embeddings * mask\n",
    "\n",
    "#Then we sum the remained of the embeddings along axis 1:\n",
    "summed = torch.sum(mask_embeddings, 1)\n",
    "\n",
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "print(summed_mask.shape)\n",
    "mean_pooled = summed / summed_mask\n",
    "db_embeded = mean_pooled.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "df = df_search\n",
    "\n",
    "token = {'input_ids': [], 'attention_mask': []}\n",
    "for index, row in df.iterrows():\n",
    "    # encode each sentence, append to dictionary\n",
    "    sentence = row['Product Name']\n",
    "    print(type(sentence))\n",
    "\n",
    "    new_token = tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                       truncation=True, padding='max_length',\n",
    "                                       return_tensors='pt')\n",
    "    token['input_ids'].append(new_token['input_ids'][0])\n",
    "    token['attention_mask'].append(new_token['attention_mask'][0])\n",
    "# reformat list of tensors to single tensor\n",
    "token['input_ids'] = torch.stack(token['input_ids'])\n",
    "token['attention_mask'] = torch.stack(token['attention_mask'])\n",
    "\n",
    "#Process tokens through model:\n",
    "output = model(**token)\n",
    "\n",
    "#The dense vector representations of text are contained within the outputs 'last_hidden_state' tensor\n",
    "embeddings = output.last_hidden_state\n",
    "\n",
    "# To perform this operation, we first resize our attention_mask tensor:\n",
    "att_mask = token['attention_mask']\n",
    "mask = att_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask_embeddings = embeddings * mask\n",
    "#Then we sum the remained of the embeddings along axis 1:\n",
    "summed = torch.sum(mask_embeddings, 1)\n",
    "\n",
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "print(summed_mask.shape)\n",
    "mean_pooled = summed / summed_mask\n",
    "db_search = mean_pooled.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "points = db_embeded\n",
    "tree = KDTree(points, leaf_size=2, metric='euclidean')              \n",
    "dist, ind = tree.query(points[:1], k=1)                \n",
    "print(ind)  # indices of 3 closest neighbors\n",
    "print(dist)  # distances to 3 closest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a6fcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.055863182426773\n"
     ]
    }
   ],
   "source": [
    "search_row = df_db.iloc[0]\n",
    "\n",
    "def get_score(a,b):\n",
    "    return 1 - abs((a-b)/(a+b))\n",
    "\n",
    "\n",
    "def Evaluator(df1, df2):\n",
    "    #df1:dnanudge database; df2:sainsburies\n",
    "    diff = 0\n",
    "    diff = diff + get_score(df1['Energy'], df2['Energy'])\n",
    "    #diff = diff + get_score(df1['Fat'], df2['Fat'])\n",
    "    diff = diff + get_score(df1['Saturated fat'], df2['Saturated fat'])\n",
    "    diff = diff + get_score(df1['Carbohydrate'], df2['Carbohydrate'])\n",
    "    diff = diff + get_score(df1['Sugar'], df2['Sugar'])\n",
    "    #diff = diff + get_score(df1['Protein'], df2['Protein'])\n",
    "    diff =diff + get_score(df1['Salt'], df2['Salt'])\n",
    "    \n",
    "    '''\n",
    "    df1['Energy'] = np.where(df1['Energy'] == df2['Energy'], 'True', 'False')\n",
    "    df1['Fat'] = np.where(df1['Fat'] == df2['Fat'], 'True', 'False')\n",
    "    df1['Saturated fat'] = np.where(df1['Saturated fat'] == df2['Saturated fat'], 'True', 'False')\n",
    "    df1['Carbohydrate'] = np.where(df1['Carbohydrate'] == df2['Carbohydrate'], 'True', 'False')\n",
    "    df1['Sugar'] = np.where(df1['Sugar'] == df2['Sugar'], 'True', 'False')\n",
    "    df1['Protein'] = np.where(df1['Protein'] == df2['Protein'], 'True', 'False')\n",
    "    df1['Salt'] = np.where(df1['Salt'] == df2['Salt'], 'True', 'False')\n",
    "    print(df1)\n",
    "    '''\n",
    "    return diff\n",
    "\n",
    "score = Evaluator(db_tree_row, search_row)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "61de9018",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2175741952.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [126]\u001b[0;36m\u001b[0m\n\u001b[0;31m    output = model(**bert_input)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "array = []\n",
    "def bert_embedding(df_db):\n",
    "    for ind, record in df_db.iterrows():\n",
    "        bert_input = tokenizer(record['Product Name'],padding='max_length', max_length = 20, \n",
    "                               truncation=True, return_tensors=\"pt\"\n",
    "                              \n",
    "        #Process tokens through model:\n",
    "        output = model(**bert_input)\n",
    "\n",
    "        #The dense vector representations of text are contained within the outputs 'last_hidden_state' tensor\n",
    "        embeddings = output.last_hidden_state)\n",
    "        print(embeddings)\n",
    "        if ind == 0:\n",
    "            array = db_embeded['input_ids'][0].detach().numpy()\n",
    "        else:\n",
    "            array= np.vstack((array,db_embeded['input_ids'][0].detach().numpy()))\n",
    "    return array\n",
    "\n",
    "def bert_series_embedding(record):\n",
    "    db_embeded = bert_input = tokenizer(record['Product Name'],padding='max_length', max_length = 20, \n",
    "                               truncation=True, return_tensors=\"pt\")\n",
    "    return db_embeded['input_ids'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding2(df):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    #Encoding:\n",
    "    sen_embeddings = model.encode(df_search['Product Name'].tolist())\n",
    "    return sen_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43a7e043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:No sentence-transformers model found with name /home/fyy/.cache/torch/sentence_transformers/bert-base-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/fyy/.cache/torch/sentence_transformers/bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7035316089162497\n",
      "141.58139729499817\n",
      "145.90612959861755\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training and Tuning Product Matcher\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "'''\n",
    "BERT Tokenizer with KD-TREE\n",
    "'''\n",
    "total = 0\n",
    "total_score = 0\n",
    "\n",
    "db_embeded = model.encode(df_db['Product Name'].tolist())\n",
    "row_sums = db_embeded.sum(axis=0)\n",
    "points = db_embeded / row_sums[np.newaxis,:]\n",
    "tree = KDTree(points, leaf_size=2, metric='euclidean')\n",
    "\n",
    "start2 = time.time()\n",
    "for index, query_product in df_search.iterrows():\n",
    "    model = SentenceTransformer('bert-base-cased')\n",
    "    #Encoding:\n",
    "\n",
    "    query_product_embeded = model.encode([query_product['Product Name']])\n",
    "\n",
    "\n",
    "    #print(query_product_embeded)\n",
    "    norm = np.linalg.norm(query_product_embeded)\n",
    "    dist, ind = tree.query(query_product_embeded/norm, k=1)\n",
    "    row_index = ind[0][0]\n",
    "    #print(row_index)\n",
    "    #print(df_db)\n",
    "    \n",
    "    db_tree_row = df_db.iloc[row_index]\n",
    "    #print(db_tree_row['Energy'] -query_product['Energy'])\n",
    "    score = Evaluator(db_tree_row, query_product)\n",
    "    total_score = total_score + score\n",
    "    total = total+5\n",
    "print(total_score/total)\n",
    "print(time.time()-start2)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c92c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5954573721244766\n",
      "36.545873165130615\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training and Tuning Product Matcher\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "'''\n",
    "Spacy with KD-TREE\n",
    "'''\n",
    "total = 0\n",
    "total_score = 0\n",
    "for index, query_product in df_search.iterrows():\n",
    "    db_embeded = spacy_embedding(df_db)\n",
    "    query_product_embeded = spacy_embedding(query_product)\n",
    "    points = db_embeded\n",
    "    tree = KDTree(points, leaf_size=2, metric='manhattan')              \n",
    "    dist, ind = tree.query(query_product_embeded, k=1)\n",
    "    row_index = ind[0][0]\n",
    "    db_tree_row = df_db.iloc[row_index]\n",
    "    #print(db_tree_row['Energy'] -query_product['Energy'])\n",
    "    score = Evaluator(db_tree_row, query_product)\n",
    "    total_score = total_score + score\n",
    "    total = total+5\n",
    "print(total_score/total)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0466e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7577947142264981\n",
      "1.2904634475708008\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "fuzzy\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "'''\n",
    "Spacy with KD-TREE\n",
    "'''\n",
    "total = 0\n",
    "total_score = 0\n",
    "for i, query_product in df_search.iterrows():\n",
    "    current_max = 0\n",
    "    index = None\n",
    "    for j, db_row in df_db.iterrows():\n",
    "        similarity = fuzz.ratio(query_product['Product Name'], db_row['Product Name'])\n",
    "        if similarity > current_max:\n",
    "            current_max = similarity\n",
    "            index = j\n",
    "        #print(db_tree_row['Energy'] -query_product['Energy'])\n",
    "    #print(index)\n",
    "    score = Evaluator(df_db.iloc[index], query_product)\n",
    "    total_score = total_score + score\n",
    "    total = total+5\n",
    "print(total_score/total)\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77aab77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7944494161511716\n",
      "249.13260793685913\n",
      "253.63945746421814\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "cosine similarity\n",
    "'''\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "'''\n",
    "cosine\n",
    "'''\n",
    "\n",
    "total = 0\n",
    "total_score = 0\n",
    "db_embeded = model.encode(df_db['Product Name'].tolist())\n",
    "start1 = time.time()\n",
    "for i, query_product in df_search.iterrows():\n",
    "    current_max = 0\n",
    "    index = None\n",
    "    for j, db_row in df_db.iterrows():\n",
    "        query_product_embeded = model.encode([query_product['Product Name']])\n",
    "        #print(db_embeded[j].reshape(1,-1).shape)\n",
    "        #print(query_product_embeded.shape)\n",
    "        similarity = cosine_similarity(query_product_embeded, db_embeded[j].reshape(1,-1))\n",
    "        if similarity > current_max:\n",
    "            current_max = similarity\n",
    "            index = j\n",
    "        #print(db_tree_row['Energy'] -query_product['Energy'])\n",
    "    #print(index)\n",
    "    score = Evaluator(df_db.iloc[index], query_product)\n",
    "    total_score = total_score + score\n",
    "    total = total+5\n",
    "print(total_score/total)\n",
    "print(time.time()-start1)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96305dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afae61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
