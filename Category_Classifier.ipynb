{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d237c5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Product Name  \\\n",
      "0         Kellogg's Rice Krispies Breakfast Cereal 1kg   \n",
      "1       Sainsbury's Berry Hooplas, Summer Edition 375g   \n",
      "2    Sainsbury's  Peach Melba Granola, Summer Editi...   \n",
      "3    Kellogg's Special K Original Breakfast Cereal ...   \n",
      "4           Kellogg's Crunchy Nut Breakfast Cereal 1kg   \n",
      "..                                                 ...   \n",
      "594  Doritos Stax Sour Cream & Onion Sharing Snacks...   \n",
      "595                  Walkers Prawn Cocktail Crisps 45g   \n",
      "596  Pringles Sizzl'N Spicy Chorizo Flavour Sharing...   \n",
      "597  Doritos Flame Grilled Steak Sharing Tortilla C...   \n",
      "598  Walkers Max KFC Kentucky Fried Chicken Sharing...   \n",
      "\n",
      "                                   Product Description Category  Energy  \\\n",
      "0    Toasted Rice Cereal Fortified with Vitamins an...   cereal   389.0   \n",
      "1    Coated cereal extrudate with natural strawberr...   cereal   395.0   \n",
      "2    A blend of honey toasted wholegrain oats with ...   cereal   437.0   \n",
      "3    Crunchy Rice, Wholewheat and Barley Flakes For...   cereal   392.0   \n",
      "4    Toasted Flakes of Corn with Sugar, Peanuts and...   cereal   398.0   \n",
      "..                                                 ...      ...     ...   \n",
      "594              Sour Cream & Onion Flavour Corn Chips   crisps   529.0   \n",
      "595  Prawn Cocktail Flavour Potato Crisps (with sug...   crisps   513.0   \n",
      "596               Spicy Chorizo Flavour Savoury Snack.   crisps   520.0   \n",
      "597             Flame Grilled Steak Flavour Corn Chips   crisps   491.0   \n",
      "598        Crispy Chicken Flavour Ridged Potato Crisps   crisps   509.0   \n",
      "\n",
      "     Saturates  Carbohydrate  Sugars  Salt  \n",
      "0          0.4          86.0     7.9  1.00  \n",
      "1          0.9          85.0    23.4  0.41  \n",
      "2          2.4          64.2    14.9  0.03  \n",
      "3          0.3          84.0    15.0  1.00  \n",
      "4          0.7          82.0    35.0  0.75  \n",
      "..         ...           ...     ...   ...  \n",
      "594        2.2          63.0     1.9  1.40  \n",
      "595        2.4          55.0     1.9  1.20  \n",
      "596        2.7          58.0     4.0  1.30  \n",
      "597        3.1          59.0     5.5  1.57  \n",
      "598        2.5          52.7     3.7  1.19  \n",
      "\n",
      "[599 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv ('data/nutrition_table_balanced.csv', encoding= 'unicode_escape')\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea8cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {'cereal':0,\n",
    "          'chocolate':1,\n",
    "         'rice':2,\n",
    "         'yogurts':3,\n",
    "         'crisps':4}\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['Category']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['Product Name']]\n",
    "\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8108120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''build bert model'''\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.Softmax()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output, dim=1)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa1a2078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479 60 60\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2074169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.802                 | Train Accuracy:  0.217                 | Val Loss:  0.791                 | Val Accuracy:  0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:58<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.774                 | Train Accuracy:  0.428                 | Val Loss:  0.757                 | Val Accuracy:  0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.730                 | Train Accuracy:  0.626                 | Val Loss:  0.720                 | Val Accuracy:  0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:01<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.695                 | Train Accuracy:  0.720                 | Val Loss:  0.691                 | Val Accuracy:  0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.663                 | Train Accuracy:  0.837                 | Val Loss:  0.657                 | Val Accuracy:  0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:35<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.630                 | Train Accuracy:  0.852                 | Val Loss:  0.623                 | Val Accuracy:  0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:44<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.598                 | Train Accuracy:  0.900                 | Val Loss:  0.595                 | Val Accuracy:  0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:21<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.573                 | Train Accuracy:  0.925                 | Val Loss:  0.574                 | Val Accuracy:  0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:27<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.552                 | Train Accuracy:  0.973                 | Val Loss:  0.556                 | Val Accuracy:  0.933\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    \n",
    "    #train_data = train_data.type(torch.LongTensor)\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 9\n",
    "model = BertClassifier(dropout=0.1)\n",
    "LR = 5.00E-07\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6794aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db = pd.read_csv('data/Sample_db.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8083acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0511, 0.0368, 0.3369, 0.0928, 0.4824],\n",
      "        [0.0457, 0.0299, 0.2440, 0.0589, 0.6215]])\n",
      "tensor([[0.0122, 0.0137, 0.0195, 0.0224, 0.9322],\n",
      "        [0.0120, 0.0091, 0.0173, 0.0233, 0.9384]])\n",
      "tensor([[0.1037, 0.0675, 0.1852, 0.5911, 0.0525],\n",
      "        [0.1052, 0.0779, 0.0934, 0.6810, 0.0425]])\n",
      "tensor([[0.0098, 0.0096, 0.0132, 0.0123, 0.9552],\n",
      "        [0.0138, 0.0112, 0.0127, 0.0164, 0.9460]])\n",
      "tensor([[0.0133, 0.0129, 0.0137, 0.0179, 0.9422],\n",
      "        [0.0623, 0.0593, 0.6081, 0.1699, 0.1003]])\n",
      "tensor([[0.0303, 0.0552, 0.7744, 0.0737, 0.0664],\n",
      "        [0.3024, 0.2093, 0.1875, 0.2569, 0.0440]])\n",
      "tensor([[0.0988, 0.0699, 0.3257, 0.3743, 0.1313],\n",
      "        [0.0485, 0.0413, 0.7814, 0.0528, 0.0761]])\n",
      "tensor([[0.1003, 0.0545, 0.4203, 0.3238, 0.1011],\n",
      "        [0.2957, 0.2346, 0.1619, 0.2643, 0.0435]])\n",
      "tensor([[0.4369, 0.1807, 0.0928, 0.2267, 0.0629],\n",
      "        [0.1656, 0.0851, 0.1320, 0.5402, 0.0771]])\n",
      "tensor([[0.1001, 0.0515, 0.2371, 0.3344, 0.2769],\n",
      "        [0.1898, 0.0800, 0.1567, 0.4481, 0.1255]])\n",
      "tensor([[0.1207, 0.0595, 0.2044, 0.5400, 0.0753],\n",
      "        [0.1443, 0.0748, 0.1283, 0.6179, 0.0346]])\n",
      "tensor([[0.1094, 0.0706, 0.3963, 0.3589, 0.0648],\n",
      "        [0.0147, 0.0084, 0.0133, 0.0153, 0.9483]])\n",
      "tensor([[0.0167, 0.0118, 0.0161, 0.0201, 0.9353],\n",
      "        [0.0091, 0.0102, 0.0116, 0.0174, 0.9518]])\n",
      "tensor([[0.3528, 0.2448, 0.0623, 0.2692, 0.0709],\n",
      "        [0.1273, 0.0685, 0.2902, 0.4191, 0.0949]])\n",
      "tensor([[0.2762, 0.1352, 0.1252, 0.4053, 0.0581],\n",
      "        [0.1637, 0.0869, 0.2346, 0.4230, 0.0918]])\n",
      "tensor([[0.3960, 0.2875, 0.0980, 0.1798, 0.0386],\n",
      "        [0.3724, 0.3406, 0.0770, 0.1363, 0.0737]])\n",
      "tensor([[0.3646, 0.2654, 0.1037, 0.2163, 0.0501],\n",
      "        [0.5064, 0.1985, 0.0778, 0.1661, 0.0512]])\n",
      "tensor([[0.0542, 0.0477, 0.7359, 0.0969, 0.0654],\n",
      "        [0.1064, 0.0569, 0.0952, 0.6918, 0.0497]])\n",
      "tensor([[0.4240, 0.2137, 0.0796, 0.2403, 0.0423],\n",
      "        [0.5317, 0.2492, 0.0561, 0.1319, 0.0311]])\n",
      "tensor([[0.0567, 0.0552, 0.6787, 0.1203, 0.0891],\n",
      "        [0.4208, 0.2567, 0.0657, 0.1969, 0.0599]])\n",
      "tensor([[0.3812, 0.2188, 0.0793, 0.2739, 0.0468],\n",
      "        [0.1739, 0.1026, 0.4221, 0.1558, 0.1456]])\n",
      "tensor([[0.5353, 0.2162, 0.0602, 0.1204, 0.0679],\n",
      "        [0.2547, 0.1840, 0.0970, 0.4081, 0.0562]])\n",
      "tensor([[0.2004, 0.1172, 0.1056, 0.5290, 0.0478],\n",
      "        [0.1245, 0.1154, 0.4484, 0.1963, 0.1154]])\n",
      "tensor([[0.4605, 0.2969, 0.0606, 0.1169, 0.0651]])\n",
      "[array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0)]\n",
      "[array(4), array(4), array(3), array(4), array(4), array(2), array(3), array(2), array(0), array(3), array(3), array(2), array(4), array(0), array(3), array(0), array(0), array(2), array(0), array(2), array(0), array(0), array(3), array(0)]\n",
      "Test Accuracy:  0.447\n",
      "Test Weighted F1 score:  0.639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.44680851063829785, 0.6386452241715399)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Multi-class evaluator\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score\n",
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            i = 0\n",
    "            output = model(input_id, mask)\n",
    "            print(output)\n",
    "            selected_output = output.argmax(dim=1)[i]#0,1,2,3,4\n",
    "            y_pred.append(selected_output.detach().numpy())\n",
    "            y_true.append(test_label[0].detach().numpy())\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "            \n",
    "            #selected = (output.argmax(dim=1) == 1).sum().item()\n",
    "\n",
    "    '''\n",
    "    Weighted F1: \n",
    "    '''\n",
    "    print(y_true)\n",
    "    print(y_pred)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    #F1 = TP/(TP + 0.5*(FP+FN))\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(f'Test Weighted F1 score: {weighted_f1: .3f}')\n",
    "    return (total_acc_test / len(test_data), weighted_f1)\n",
    "    \n",
    "evaluate(model, df_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d69d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479 60 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:50<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.807                 | Train Accuracy:  0.225                 | Val Loss:  0.807                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:54<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.807                 | Train Accuracy:  0.221                 | Val Loss:  0.807                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:48<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.805                 | Train Accuracy:  0.240                 | Val Loss:  0.808                 | Val Accuracy:  0.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:56<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.806                 | Train Accuracy:  0.213                 | Val Loss:  0.807                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:57<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.809                 | Train Accuracy:  0.209                 | Val Loss:  0.805                 | Val Accuracy:  0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.808                 | Train Accuracy:  0.196                 | Val Loss:  0.807                 | Val Accuracy:  0.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:34<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.808                 | Train Accuracy:  0.221                 | Val Loss:  0.808                 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:50<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.808                 | Train Accuracy:  0.198                 | Val Loss:  0.811                 | Val Accuracy:  0.167\n",
      "tensor([[0.1275, 0.2706, 0.2852, 0.1956, 0.1212],\n",
      "        [0.1176, 0.1816, 0.2693, 0.3394, 0.0921]])\n",
      "tensor([[0.0747, 0.1772, 0.3515, 0.2407, 0.1559],\n",
      "        [0.0823, 0.0708, 0.2865, 0.2793, 0.2811]])\n",
      "tensor([[0.1533, 0.1512, 0.1322, 0.3810, 0.1823],\n",
      "        [0.1640, 0.1521, 0.2888, 0.2768, 0.1182]])\n",
      "tensor([[0.1384, 0.1471, 0.2830, 0.3081, 0.1234],\n",
      "        [0.1552, 0.2695, 0.2434, 0.1983, 0.1336]])\n",
      "tensor([[0.2865, 0.1191, 0.3749, 0.1368, 0.0826],\n",
      "        [0.1602, 0.1271, 0.3972, 0.1541, 0.1613]])\n",
      "tensor([[0.1186, 0.1388, 0.3831, 0.1665, 0.1930],\n",
      "        [0.1485, 0.1823, 0.3398, 0.2045, 0.1248]])\n",
      "tensor([[0.0680, 0.2547, 0.3365, 0.2010, 0.1398],\n",
      "        [0.1378, 0.1594, 0.3054, 0.2327, 0.1647]])\n",
      "tensor([[0.1752, 0.1337, 0.3356, 0.2384, 0.1171],\n",
      "        [0.1882, 0.1276, 0.3448, 0.1986, 0.1409]])\n",
      "tensor([[0.1161, 0.1768, 0.2606, 0.2958, 0.1507],\n",
      "        [0.1782, 0.1485, 0.2752, 0.2887, 0.1095]])\n",
      "tensor([[0.1550, 0.1824, 0.3043, 0.2423, 0.1160],\n",
      "        [0.1116, 0.1887, 0.2689, 0.2833, 0.1476]])\n",
      "tensor([[0.1307, 0.1621, 0.3241, 0.2058, 0.1772],\n",
      "        [0.1022, 0.1548, 0.3350, 0.2280, 0.1800]])\n",
      "tensor([[0.0953, 0.1477, 0.3618, 0.1849, 0.2104],\n",
      "        [0.1817, 0.1339, 0.2866, 0.2241, 0.1737]])\n",
      "tensor([[0.1521, 0.1697, 0.3983, 0.1686, 0.1113],\n",
      "        [0.1156, 0.1386, 0.2155, 0.4216, 0.1087]])\n",
      "tensor([[0.1982, 0.1939, 0.2623, 0.1865, 0.1591],\n",
      "        [0.1334, 0.2421, 0.2739, 0.2295, 0.1211]])\n",
      "tensor([[0.0805, 0.1416, 0.4429, 0.2097, 0.1253],\n",
      "        [0.1887, 0.2343, 0.2332, 0.1554, 0.1883]])\n",
      "tensor([[0.1868, 0.1896, 0.2793, 0.1709, 0.1734],\n",
      "        [0.1214, 0.1292, 0.2524, 0.3410, 0.1561]])\n",
      "tensor([[0.0970, 0.2417, 0.2723, 0.2382, 0.1507],\n",
      "        [0.1957, 0.2622, 0.2482, 0.1454, 0.1485]])\n",
      "tensor([[0.1338, 0.2731, 0.3419, 0.1574, 0.0938],\n",
      "        [0.2065, 0.1449, 0.3019, 0.1913, 0.1555]])\n",
      "tensor([[0.1207, 0.2204, 0.3893, 0.1755, 0.0942],\n",
      "        [0.1619, 0.1670, 0.3052, 0.2577, 0.1082]])\n",
      "tensor([[0.2269, 0.2148, 0.2341, 0.1977, 0.1265],\n",
      "        [0.0759, 0.1302, 0.4341, 0.1558, 0.2040]])\n",
      "tensor([[0.1215, 0.1541, 0.3669, 0.1981, 0.1594],\n",
      "        [0.1395, 0.1763, 0.2887, 0.2496, 0.1459]])\n",
      "tensor([[0.1508, 0.1647, 0.2604, 0.2319, 0.1923],\n",
      "        [0.0974, 0.1903, 0.2783, 0.2957, 0.1383]])\n",
      "tensor([[0.2085, 0.1713, 0.2030, 0.2824, 0.1348],\n",
      "        [0.1016, 0.1349, 0.3620, 0.2882, 0.1133]])\n",
      "tensor([[0.1806, 0.1756, 0.1901, 0.2473, 0.2063],\n",
      "        [0.0829, 0.1096, 0.3883, 0.2217, 0.1976]])\n",
      "tensor([[0.1461, 0.1854, 0.2013, 0.3443, 0.1229],\n",
      "        [0.1040, 0.2619, 0.2917, 0.1799, 0.1625]])\n",
      "tensor([[0.1165, 0.1251, 0.2966, 0.3005, 0.1613],\n",
      "        [0.1412, 0.1920, 0.2541, 0.2950, 0.1177]])\n",
      "tensor([[0.1658, 0.1625, 0.2099, 0.3202, 0.1416],\n",
      "        [0.1898, 0.1177, 0.3307, 0.2299, 0.1319]])\n",
      "tensor([[0.2885, 0.2397, 0.1811, 0.2096, 0.0811],\n",
      "        [0.1232, 0.1242, 0.2757, 0.3504, 0.1265]])\n",
      "tensor([[0.1906, 0.1989, 0.3220, 0.1588, 0.1297],\n",
      "        [0.1652, 0.1347, 0.2614, 0.2892, 0.1494]])\n",
      "tensor([[0.1214, 0.1622, 0.3764, 0.1914, 0.1485],\n",
      "        [0.1238, 0.2126, 0.3056, 0.2232, 0.1348]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(2), array(2), array(3), array(3), array(2), array(2), array(2), array(2), array(3), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(3), array(3), array(3), array(3), array(3), array(0), array(2), array(2)]\n",
      "Test Accuracy:  0.133\n",
      "Test Weighted F1 score:  0.116\n",
      "EPOCHS: 8 | LR:5e-10 | DROPOUT:0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:00<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.804                 | Train Accuracy:  0.213                 | Val Loss:  0.797                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.764                 | Train Accuracy:  0.486                 | Val Loss:  0.726                 | Val Accuracy:  0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:57<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.698                 | Train Accuracy:  0.724                 | Val Loss:  0.681                 | Val Accuracy:  0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:04<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.656                 | Train Accuracy:  0.816                 | Val Loss:  0.643                 | Val Accuracy:  0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:56<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.620                 | Train Accuracy:  0.843                 | Val Loss:  0.605                 | Val Accuracy:  0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:02<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.584                 | Train Accuracy:  0.910                 | Val Loss:  0.575                 | Val Accuracy:  0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.558                 | Train Accuracy:  0.950                 | Val Loss:  0.553                 | Val Accuracy:  0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:04<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.540                 | Train Accuracy:  0.969                 | Val Loss:  0.540                 | Val Accuracy:  0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:55<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.525                 | Train Accuracy:  0.977                 | Val Loss:  0.525                 | Val Accuracy:  0.967\n",
      "tensor([[0.0382, 0.0299, 0.0153, 0.8892, 0.0274],\n",
      "        [0.1066, 0.0435, 0.0191, 0.7901, 0.0406]])\n",
      "tensor([[0.0303, 0.0288, 0.0135, 0.8982, 0.0291],\n",
      "        [0.3426, 0.2123, 0.2205, 0.1256, 0.0989]])\n",
      "tensor([[0.0090, 0.0080, 0.0089, 0.0093, 0.9648],\n",
      "        [0.5926, 0.1574, 0.0794, 0.1090, 0.0617]])\n",
      "tensor([[0.1571, 0.4540, 0.2779, 0.0756, 0.0355],\n",
      "        [0.6864, 0.0886, 0.0847, 0.0693, 0.0711]])\n",
      "tensor([[0.0099, 0.0118, 0.0084, 0.0165, 0.9534],\n",
      "        [0.0459, 0.0386, 0.0194, 0.8627, 0.0335]])\n",
      "tensor([[0.0667, 0.0852, 0.0262, 0.7896, 0.0322],\n",
      "        [0.0316, 0.0253, 0.0133, 0.9094, 0.0203]])\n",
      "tensor([[0.5183, 0.1861, 0.1686, 0.0741, 0.0528],\n",
      "        [0.0176, 0.0218, 0.9362, 0.0088, 0.0155]])\n",
      "tensor([[0.0172, 0.0239, 0.9373, 0.0061, 0.0155],\n",
      "        [0.0171, 0.0216, 0.9350, 0.0099, 0.0163]])\n",
      "tensor([[0.6424, 0.1090, 0.0997, 0.0859, 0.0630],\n",
      "        [0.0123, 0.0174, 0.9511, 0.0080, 0.0112]])\n",
      "tensor([[0.0491, 0.0486, 0.0227, 0.8440, 0.0356],\n",
      "        [0.0266, 0.0214, 0.9231, 0.0131, 0.0158]])\n",
      "tensor([[0.0545, 0.8866, 0.0187, 0.0219, 0.0183],\n",
      "        [0.0109, 0.0087, 0.0080, 0.0083, 0.9642]])\n",
      "tensor([[0.0086, 0.0102, 0.0075, 0.0088, 0.9650],\n",
      "        [0.0080, 0.0094, 0.0076, 0.0084, 0.9667]])\n",
      "tensor([[0.0112, 0.0106, 0.0089, 0.0093, 0.9600],\n",
      "        [0.0205, 0.0181, 0.9329, 0.0108, 0.0177]])\n",
      "tensor([[0.0092, 0.0077, 0.0071, 0.0081, 0.9680],\n",
      "        [0.0291, 0.9108, 0.0160, 0.0267, 0.0173]])\n",
      "tensor([[0.0374, 0.0232, 0.0131, 0.8903, 0.0360],\n",
      "        [0.0146, 0.0188, 0.0101, 0.0164, 0.9401]])\n",
      "tensor([[0.3905, 0.2254, 0.1560, 0.1645, 0.0636],\n",
      "        [0.0394, 0.0282, 0.0168, 0.8872, 0.0285]])\n",
      "tensor([[0.0120, 0.0116, 0.0132, 0.0107, 0.9524],\n",
      "        [0.0218, 0.0431, 0.9038, 0.0092, 0.0222]])\n",
      "tensor([[0.5468, 0.1664, 0.0846, 0.1297, 0.0726],\n",
      "        [0.0177, 0.0197, 0.9305, 0.0121, 0.0199]])\n",
      "tensor([[0.0428, 0.0364, 0.0213, 0.8780, 0.0215],\n",
      "        [0.0270, 0.9384, 0.0125, 0.0119, 0.0102]])\n",
      "tensor([[0.0209, 0.0278, 0.9237, 0.0118, 0.0158],\n",
      "        [0.0257, 0.9223, 0.0165, 0.0187, 0.0168]])\n",
      "tensor([[0.0455, 0.0288, 0.0200, 0.8715, 0.0342],\n",
      "        [0.0311, 0.0254, 0.0173, 0.9076, 0.0185]])\n",
      "tensor([[0.0124, 0.0129, 0.0117, 0.0174, 0.9456],\n",
      "        [0.0227, 0.0375, 0.9030, 0.0118, 0.0251]])\n",
      "tensor([[0.0190, 0.0309, 0.9213, 0.0117, 0.0171],\n",
      "        [0.0268, 0.9268, 0.0164, 0.0187, 0.0113]])\n",
      "tensor([[0.5770, 0.1221, 0.1055, 0.1089, 0.0864],\n",
      "        [0.0397, 0.0375, 0.0201, 0.8711, 0.0317]])\n",
      "tensor([[0.6798, 0.1029, 0.0708, 0.0801, 0.0665],\n",
      "        [0.0445, 0.0426, 0.0177, 0.8662, 0.0290]])\n",
      "tensor([[0.0232, 0.0334, 0.9090, 0.0119, 0.0224],\n",
      "        [0.0266, 0.9311, 0.0136, 0.0196, 0.0091]])\n",
      "tensor([[0.0426, 0.0285, 0.0150, 0.8893, 0.0246],\n",
      "        [0.1441, 0.4531, 0.3096, 0.0575, 0.0356]])\n",
      "tensor([[0.5790, 0.1201, 0.1726, 0.0636, 0.0647],\n",
      "        [0.4368, 0.1687, 0.1068, 0.1920, 0.0957]])\n",
      "tensor([[0.6346, 0.1217, 0.1052, 0.0826, 0.0558],\n",
      "        [0.0198, 0.0221, 0.9375, 0.0077, 0.0129]])\n",
      "tensor([[0.0376, 0.0273, 0.0166, 0.8846, 0.0339],\n",
      "        [0.6007, 0.0781, 0.1243, 0.1024, 0.0945]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "Test Accuracy:  0.983\n",
      "Test Weighted F1 score:  1.000\n",
      "EPOCHS: 9 | LR:5e-07 | DROPOUT:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.803                 | Train Accuracy:  0.230                 | Val Loss:  0.791                 | Val Accuracy:  0.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.790                 | Train Accuracy:  0.334                 | Val Loss:  0.790                 | Val Accuracy:  0.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:52<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.780                 | Train Accuracy:  0.382                 | Val Loss:  0.775                 | Val Accuracy:  0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:57<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.758                 | Train Accuracy:  0.472                 | Val Loss:  0.757                 | Val Accuracy:  0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:54<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.726                 | Train Accuracy:  0.601                 | Val Loss:  0.728                 | Val Accuracy:  0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:50<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.694                 | Train Accuracy:  0.660                 | Val Loss:  0.691                 | Val Accuracy:  0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:56<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.669                 | Train Accuracy:  0.674                 | Val Loss:  0.676                 | Val Accuracy:  0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:47<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.641                 | Train Accuracy:  0.770                 | Val Loss:  0.639                 | Val Accuracy:  0.867\n",
      "tensor([[0.1328, 0.1690, 0.2821, 0.2929, 0.1232],\n",
      "        [0.6898, 0.1403, 0.0504, 0.0634, 0.0560]])\n",
      "tensor([[0.1795, 0.1836, 0.3057, 0.1294, 0.2019],\n",
      "        [0.6188, 0.0639, 0.0807, 0.1012, 0.1354]])\n",
      "tensor([[0.0893, 0.0720, 0.0472, 0.0637, 0.7279],\n",
      "        [0.7401, 0.0514, 0.0490, 0.0660, 0.0935]])\n",
      "tensor([[0.1094, 0.1837, 0.3229, 0.2198, 0.1642],\n",
      "        [0.7910, 0.0794, 0.0157, 0.0690, 0.0450]])\n",
      "tensor([[0.0520, 0.0420, 0.0200, 0.0394, 0.8466],\n",
      "        [0.0307, 0.1277, 0.1802, 0.4867, 0.1747]])\n",
      "tensor([[0.0574, 0.1900, 0.2712, 0.2329, 0.2485],\n",
      "        [0.0713, 0.1387, 0.3260, 0.2800, 0.1840]])\n",
      "tensor([[0.5629, 0.1377, 0.1093, 0.0744, 0.1156],\n",
      "        [0.0532, 0.0345, 0.8386, 0.0331, 0.0406]])\n",
      "tensor([[0.0332, 0.0301, 0.8662, 0.0293, 0.0412],\n",
      "        [0.0339, 0.0304, 0.8442, 0.0385, 0.0530]])\n",
      "tensor([[0.8379, 0.0291, 0.0342, 0.0544, 0.0443],\n",
      "        [0.0302, 0.0528, 0.8465, 0.0216, 0.0489]])\n",
      "tensor([[0.0470, 0.1355, 0.2243, 0.4460, 0.1472],\n",
      "        [0.0436, 0.0251, 0.8530, 0.0244, 0.0538]])\n",
      "tensor([[0.1033, 0.4265, 0.1677, 0.0879, 0.2146],\n",
      "        [0.0991, 0.0478, 0.0372, 0.0357, 0.7802]])\n",
      "tensor([[0.0339, 0.0693, 0.0334, 0.0296, 0.8337],\n",
      "        [0.1208, 0.0445, 0.0223, 0.0464, 0.7661]])\n",
      "tensor([[0.0918, 0.0640, 0.0366, 0.0436, 0.7640],\n",
      "        [0.0239, 0.0136, 0.8954, 0.0193, 0.0478]])\n",
      "tensor([[0.0326, 0.0518, 0.0643, 0.0417, 0.8096],\n",
      "        [0.3102, 0.2326, 0.1124, 0.0592, 0.2856]])\n",
      "tensor([[0.1312, 0.2358, 0.3107, 0.1108, 0.2115],\n",
      "        [0.0254, 0.0694, 0.0265, 0.0319, 0.8467]])\n",
      "tensor([[0.7394, 0.0690, 0.0345, 0.0484, 0.1087],\n",
      "        [0.0503, 0.0993, 0.5229, 0.2009, 0.1266]])\n",
      "tensor([[0.0360, 0.0492, 0.0385, 0.0408, 0.8355],\n",
      "        [0.0284, 0.0118, 0.8161, 0.0606, 0.0831]])\n",
      "tensor([[0.6836, 0.1107, 0.0296, 0.0850, 0.0911],\n",
      "        [0.0396, 0.0420, 0.8382, 0.0251, 0.0551]])\n",
      "tensor([[0.1357, 0.1083, 0.2280, 0.2377, 0.2902],\n",
      "        [0.1220, 0.4494, 0.1308, 0.2053, 0.0925]])\n",
      "tensor([[0.0511, 0.0425, 0.7763, 0.0728, 0.0573],\n",
      "        [0.0966, 0.4661, 0.0613, 0.1791, 0.1969]])\n",
      "tensor([[0.0748, 0.0926, 0.4294, 0.2169, 0.1864],\n",
      "        [0.0903, 0.0654, 0.1913, 0.3743, 0.2788]])\n",
      "tensor([[0.0344, 0.0544, 0.0477, 0.0856, 0.7779],\n",
      "        [0.0776, 0.1069, 0.6768, 0.0677, 0.0711]])\n",
      "tensor([[0.0368, 0.0472, 0.8335, 0.0400, 0.0425],\n",
      "        [0.0818, 0.6090, 0.1054, 0.0936, 0.1102]])\n",
      "tensor([[0.6861, 0.0658, 0.0657, 0.0696, 0.1128],\n",
      "        [0.1452, 0.1725, 0.3079, 0.1651, 0.2094]])\n",
      "tensor([[0.8759, 0.0396, 0.0246, 0.0307, 0.0292],\n",
      "        [0.0994, 0.1827, 0.2837, 0.2417, 0.1926]])\n",
      "tensor([[0.0457, 0.0343, 0.8296, 0.0373, 0.0530],\n",
      "        [0.0920, 0.3053, 0.2225, 0.1289, 0.2513]])\n",
      "tensor([[0.0927, 0.1708, 0.2108, 0.4272, 0.0985],\n",
      "        [0.0894, 0.0689, 0.6993, 0.0611, 0.0813]])\n",
      "tensor([[0.8263, 0.0448, 0.0346, 0.0373, 0.0569],\n",
      "        [0.8499, 0.0402, 0.0326, 0.0400, 0.0374]])\n",
      "tensor([[0.8476, 0.0507, 0.0143, 0.0542, 0.0332],\n",
      "        [0.0319, 0.1013, 0.6814, 0.1099, 0.0754]])\n",
      "tensor([[0.0485, 0.1959, 0.2462, 0.3917, 0.1177],\n",
      "        [0.7719, 0.1002, 0.0473, 0.0215, 0.0591]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(3), array(2), array(4), array(2), array(4), array(2), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(2), array(0), array(4), array(0), array(4), array(2), array(2), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "Test Accuracy:  0.800\n",
      "Test Weighted F1 score:  0.796\n",
      "EPOCHS: 8 | LR:5e-07 | DROPOUT:0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:51<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.809                 | Train Accuracy:  0.177                 | Val Loss:  0.811                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:45<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.808                 | Train Accuracy:  0.211                 | Val Loss:  0.813                 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:49<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.809                 | Train Accuracy:  0.173                 | Val Loss:  0.804                 | Val Accuracy:  0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:53<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.807                 | Train Accuracy:  0.196                 | Val Loss:  0.811                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:56<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.808                 | Train Accuracy:  0.186                 | Val Loss:  0.807                 | Val Accuracy:  0.183\n",
      "tensor([[0.2944, 0.1353, 0.2295, 0.1792, 0.1615],\n",
      "        [0.3019, 0.1013, 0.2951, 0.1411, 0.1606]])\n",
      "tensor([[0.4150, 0.1150, 0.1732, 0.1461, 0.1507],\n",
      "        [0.2798, 0.1056, 0.3252, 0.1318, 0.1575]])\n",
      "tensor([[0.3710, 0.1049, 0.1940, 0.1478, 0.1823],\n",
      "        [0.2381, 0.1128, 0.2439, 0.1681, 0.2371]])\n",
      "tensor([[0.2275, 0.1056, 0.2077, 0.1581, 0.3011],\n",
      "        [0.3155, 0.0794, 0.1878, 0.1508, 0.2665]])\n",
      "tensor([[0.3242, 0.1007, 0.2206, 0.1382, 0.2163],\n",
      "        [0.2561, 0.1098, 0.2707, 0.1522, 0.2113]])\n",
      "tensor([[0.2428, 0.1802, 0.2104, 0.1669, 0.1996],\n",
      "        [0.3374, 0.1230, 0.1930, 0.1571, 0.1895]])\n",
      "tensor([[0.2939, 0.1398, 0.2068, 0.1930, 0.1665],\n",
      "        [0.2536, 0.2233, 0.2091, 0.1345, 0.1794]])\n",
      "tensor([[0.3279, 0.0962, 0.2460, 0.1438, 0.1861],\n",
      "        [0.1790, 0.1014, 0.3146, 0.2239, 0.1811]])\n",
      "tensor([[0.2585, 0.0909, 0.2076, 0.1804, 0.2626],\n",
      "        [0.3912, 0.1524, 0.2007, 0.1404, 0.1153]])\n",
      "tensor([[0.2392, 0.1073, 0.2658, 0.1532, 0.2346],\n",
      "        [0.2439, 0.0915, 0.2157, 0.1936, 0.2552]])\n",
      "tensor([[0.2658, 0.1280, 0.2033, 0.1824, 0.2205],\n",
      "        [0.3202, 0.1269, 0.2093, 0.1867, 0.1569]])\n",
      "tensor([[0.3043, 0.1074, 0.2612, 0.1434, 0.1837],\n",
      "        [0.3001, 0.1288, 0.1657, 0.1605, 0.2448]])\n",
      "tensor([[0.3321, 0.1047, 0.1613, 0.1966, 0.2053],\n",
      "        [0.2238, 0.0795, 0.2655, 0.2031, 0.2280]])\n",
      "tensor([[0.3426, 0.1272, 0.2244, 0.2026, 0.1033],\n",
      "        [0.2139, 0.0933, 0.2719, 0.1876, 0.2333]])\n",
      "tensor([[0.2632, 0.1030, 0.1718, 0.2304, 0.2316],\n",
      "        [0.3271, 0.0930, 0.2173, 0.1400, 0.2226]])\n",
      "tensor([[0.2830, 0.1065, 0.2697, 0.1297, 0.2110],\n",
      "        [0.2522, 0.1467, 0.1667, 0.1526, 0.2818]])\n",
      "tensor([[0.3503, 0.0876, 0.1845, 0.1666, 0.2110],\n",
      "        [0.2896, 0.1374, 0.2103, 0.1619, 0.2007]])\n",
      "tensor([[0.3339, 0.1006, 0.1669, 0.1409, 0.2577],\n",
      "        [0.2563, 0.1276, 0.1831, 0.1812, 0.2517]])\n",
      "tensor([[0.3395, 0.1029, 0.1653, 0.1541, 0.2383],\n",
      "        [0.3309, 0.1226, 0.2429, 0.1386, 0.1650]])\n",
      "tensor([[0.2686, 0.1487, 0.2576, 0.1289, 0.1961],\n",
      "        [0.3579, 0.0961, 0.2474, 0.1290, 0.1695]])\n",
      "tensor([[0.2431, 0.0958, 0.2456, 0.1304, 0.2850],\n",
      "        [0.3446, 0.0771, 0.2141, 0.1551, 0.2091]])\n",
      "tensor([[0.4157, 0.1280, 0.1464, 0.1640, 0.1459],\n",
      "        [0.2001, 0.1192, 0.2274, 0.2481, 0.2052]])\n",
      "tensor([[0.2572, 0.1669, 0.2200, 0.1740, 0.1819],\n",
      "        [0.3248, 0.1644, 0.1684, 0.1801, 0.1623]])\n",
      "tensor([[0.2880, 0.0953, 0.2023, 0.1592, 0.2552],\n",
      "        [0.3030, 0.1196, 0.1922, 0.1839, 0.2012]])\n",
      "tensor([[0.3610, 0.0745, 0.2774, 0.1069, 0.1801],\n",
      "        [0.2571, 0.0760, 0.2991, 0.1253, 0.2425]])\n",
      "tensor([[0.2778, 0.1455, 0.1750, 0.2406, 0.1611],\n",
      "        [0.1917, 0.1786, 0.1559, 0.2302, 0.2435]])\n",
      "tensor([[0.2961, 0.1394, 0.1956, 0.2268, 0.1421],\n",
      "        [0.3021, 0.1312, 0.1816, 0.1839, 0.2012]])\n",
      "tensor([[0.3610, 0.1025, 0.2071, 0.1450, 0.1844],\n",
      "        [0.2808, 0.1210, 0.1652, 0.2003, 0.2327]])\n",
      "tensor([[0.2541, 0.0865, 0.2986, 0.1416, 0.2193],\n",
      "        [0.3508, 0.0966, 0.1994, 0.1755, 0.1777]])\n",
      "tensor([[0.3067, 0.1328, 0.2492, 0.1696, 0.1417],\n",
      "        [0.3246, 0.1205, 0.2404, 0.1239, 0.1905]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(0), array(0), array(0), array(4), array(0), array(0), array(0), array(0), array(4), array(2), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(4), array(0), array(0), array(0), array(0), array(0), array(0), array(0), array(2), array(0)]\n",
      "Test Accuracy:  0.200\n",
      "Test Weighted F1 score:  0.097\n",
      "EPOCHS: 5 | LR:1e-08 | DROPOUT:0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [11:35<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.851                 | Train Accuracy:  0.205                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:00<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [11:51<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:04<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:12<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:05<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:04<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:05<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:04<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [12:14<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.851                 | Train Accuracy:  0.207                 | Val Loss:  0.861                 | Val Accuracy:  0.183\n",
      "tensor([[3.7531e-10, 1.4089e-20, 6.4974e-21, 3.6832e-21, 1.0000e+00],\n",
      "        [2.0437e-10, 2.3940e-20, 1.6069e-20, 7.3912e-21, 1.0000e+00]])\n",
      "tensor([[4.9830e-10, 4.1798e-20, 1.9963e-20, 1.9521e-20, 1.0000e+00],\n",
      "        [2.1394e-10, 4.9289e-21, 2.3527e-21, 2.4114e-21, 1.0000e+00]])\n",
      "tensor([[4.6662e-10, 1.0600e-19, 4.4452e-20, 3.2331e-20, 1.0000e+00],\n",
      "        [4.9224e-10, 3.1646e-20, 1.0487e-20, 7.7977e-21, 1.0000e+00]])\n",
      "tensor([[3.9487e-10, 1.9140e-19, 7.4985e-20, 5.8248e-20, 1.0000e+00],\n",
      "        [3.1841e-10, 1.0422e-20, 6.9469e-21, 4.6580e-21, 1.0000e+00]])\n",
      "tensor([[3.0874e-10, 3.7820e-20, 1.8139e-20, 2.3611e-20, 1.0000e+00],\n",
      "        [7.6653e-10, 4.2685e-20, 2.8928e-20, 1.7980e-20, 1.0000e+00]])\n",
      "tensor([[7.6523e-10, 2.9511e-19, 2.0120e-19, 1.3729e-19, 1.0000e+00],\n",
      "        [6.1100e-10, 3.7962e-20, 2.1473e-20, 1.4773e-20, 1.0000e+00]])\n",
      "tensor([[1.2971e-09, 1.8751e-19, 1.3360e-19, 1.0512e-19, 1.0000e+00],\n",
      "        [5.1110e-10, 2.6707e-20, 1.6193e-20, 1.2295e-20, 1.0000e+00]])\n",
      "tensor([[2.7131e-10, 2.5780e-20, 1.4150e-20, 1.2516e-20, 1.0000e+00],\n",
      "        [2.9353e-10, 3.9162e-20, 2.0835e-20, 1.8348e-20, 1.0000e+00]])\n",
      "tensor([[3.0416e-10, 2.5926e-20, 1.3384e-20, 1.2837e-20, 1.0000e+00],\n",
      "        [3.8926e-10, 1.6257e-20, 1.0042e-20, 7.0850e-21, 1.0000e+00]])\n",
      "tensor([[2.7392e-10, 2.9562e-20, 1.9170e-20, 2.1594e-20, 1.0000e+00],\n",
      "        [1.3284e-09, 4.6159e-20, 4.1441e-20, 2.3459e-20, 1.0000e+00]])\n",
      "tensor([[4.2834e-10, 2.7387e-19, 2.2052e-19, 1.6795e-19, 1.0000e+00],\n",
      "        [6.5738e-10, 6.7717e-20, 4.4192e-20, 3.5285e-20, 1.0000e+00]])\n",
      "tensor([[6.2531e-10, 3.7134e-20, 1.3377e-20, 1.6852e-20, 1.0000e+00],\n",
      "        [4.6151e-10, 2.6997e-20, 1.0348e-20, 7.9900e-21, 1.0000e+00]])\n",
      "tensor([[1.3848e-10, 4.4264e-21, 2.9907e-21, 1.5123e-21, 1.0000e+00],\n",
      "        [3.7604e-10, 5.2014e-20, 2.8828e-20, 2.5695e-20, 1.0000e+00]])\n",
      "tensor([[6.0720e-10, 1.8600e-19, 1.0387e-19, 7.5021e-20, 1.0000e+00],\n",
      "        [3.4216e-10, 3.5191e-20, 2.0749e-20, 1.6168e-20, 1.0000e+00]])\n",
      "tensor([[1.5515e-10, 1.9669e-20, 1.1773e-20, 1.0005e-20, 1.0000e+00],\n",
      "        [1.7863e-10, 6.1375e-21, 3.1609e-21, 1.7379e-21, 1.0000e+00]])\n",
      "tensor([[2.3066e-10, 1.9238e-20, 8.6836e-21, 7.0410e-21, 1.0000e+00],\n",
      "        [3.6567e-10, 6.3446e-20, 3.8266e-20, 3.2746e-20, 1.0000e+00]])\n",
      "tensor([[7.9043e-10, 4.3693e-20, 1.8981e-20, 1.6074e-20, 1.0000e+00],\n",
      "        [2.6236e-10, 1.1450e-20, 4.5877e-21, 4.9531e-21, 1.0000e+00]])\n",
      "tensor([[5.9240e-10, 8.2130e-20, 4.1425e-20, 2.5034e-20, 1.0000e+00],\n",
      "        [1.5424e-10, 1.9055e-20, 1.1211e-20, 5.4147e-21, 1.0000e+00]])\n",
      "tensor([[6.7141e-10, 7.8001e-20, 5.3048e-20, 2.9217e-20, 1.0000e+00],\n",
      "        [3.9338e-10, 1.5552e-20, 7.4128e-21, 5.4295e-21, 1.0000e+00]])\n",
      "tensor([[4.0846e-10, 2.6789e-20, 1.5328e-20, 1.1577e-20, 1.0000e+00],\n",
      "        [1.5251e-10, 2.7282e-20, 1.9894e-20, 1.0485e-20, 1.0000e+00]])\n",
      "tensor([[2.1714e-10, 3.2748e-20, 1.6213e-20, 1.4596e-20, 1.0000e+00],\n",
      "        [6.0346e-10, 1.1845e-19, 4.1913e-20, 3.4025e-20, 1.0000e+00]])\n",
      "tensor([[5.8919e-10, 5.9158e-20, 2.6385e-20, 2.3532e-20, 1.0000e+00],\n",
      "        [4.3625e-10, 1.1467e-19, 5.5886e-20, 5.6970e-20, 1.0000e+00]])\n",
      "tensor([[5.6521e-10, 6.8528e-20, 4.7252e-20, 4.1624e-20, 1.0000e+00],\n",
      "        [5.4838e-10, 9.0824e-20, 4.8399e-20, 3.6516e-20, 1.0000e+00]])\n",
      "tensor([[6.0903e-10, 2.4645e-20, 9.8387e-21, 1.0593e-20, 1.0000e+00],\n",
      "        [5.0077e-10, 1.0479e-19, 4.7005e-20, 4.1782e-20, 1.0000e+00]])\n",
      "tensor([[3.7138e-10, 1.5711e-20, 8.6441e-21, 5.3940e-21, 1.0000e+00],\n",
      "        [1.6369e-10, 5.7974e-21, 2.6802e-21, 2.5132e-21, 1.0000e+00]])\n",
      "tensor([[4.4518e-10, 2.9395e-20, 2.3030e-20, 8.9224e-21, 1.0000e+00],\n",
      "        [9.2183e-11, 1.2118e-20, 4.4394e-21, 3.4816e-21, 1.0000e+00]])\n",
      "tensor([[4.9693e-10, 1.4846e-20, 8.6049e-21, 6.3140e-21, 1.0000e+00],\n",
      "        [1.0215e-09, 2.1472e-20, 1.2281e-20, 1.2552e-20, 1.0000e+00]])\n",
      "tensor([[3.0203e-10, 9.0343e-21, 3.6164e-21, 3.8384e-21, 1.0000e+00],\n",
      "        [2.6180e-10, 2.8159e-20, 1.5095e-20, 1.2796e-20, 1.0000e+00]])\n",
      "tensor([[8.9318e-10, 9.7327e-20, 6.4851e-20, 3.7420e-20, 1.0000e+00],\n",
      "        [3.0449e-10, 1.1331e-20, 4.7190e-21, 4.9239e-21, 1.0000e+00]])\n",
      "tensor([[7.5304e-10, 5.8270e-20, 2.7001e-20, 2.2408e-20, 1.0000e+00],\n",
      "        [5.4975e-10, 1.3050e-19, 1.1163e-19, 6.6809e-20, 1.0000e+00]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4), array(4)]\n",
      "Test Accuracy:  0.167\n",
      "Test Weighted F1 score:  0.088\n",
      "EPOCHS: 10 | LR:0.01 | DROPOUT:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [15:00<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.854                 | Train Accuracy:  0.198                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [15:48<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.853                 | Train Accuracy:  0.203                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [15:50<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.853                 | Train Accuracy:  0.203                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [16:09<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.853                 | Train Accuracy:  0.203                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [15:53<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.853                 | Train Accuracy:  0.203                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [16:11<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.853                 | Train Accuracy:  0.203                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [15:55<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.853                 | Train Accuracy:  0.203                 | Val Loss:  0.836                 | Val Accuracy:  0.233\n",
      "tensor([[4.0334e-19, 1.0000e+00, 7.0072e-18, 8.2008e-18, 6.2178e-19],\n",
      "        [4.8867e-21, 1.0000e+00, 8.5602e-21, 1.0849e-18, 3.0747e-21]])\n",
      "tensor([[8.0889e-19, 1.0000e+00, 5.4257e-18, 2.3585e-17, 1.2704e-18],\n",
      "        [1.0047e-18, 1.0000e+00, 1.4908e-18, 9.4058e-18, 4.5103e-19]])\n",
      "tensor([[5.0360e-19, 1.0000e+00, 9.6400e-18, 5.6197e-18, 2.0946e-18],\n",
      "        [2.5488e-17, 1.0000e+00, 9.2536e-17, 2.6295e-16, 1.4663e-17]])\n",
      "tensor([[1.2373e-17, 1.0000e+00, 3.9482e-17, 1.5821e-16, 1.7661e-17],\n",
      "        [1.7073e-19, 1.0000e+00, 8.7395e-19, 9.4026e-17, 1.9669e-19]])\n",
      "tensor([[5.7371e-18, 1.0000e+00, 1.8073e-17, 7.1401e-17, 3.9517e-18],\n",
      "        [5.6891e-19, 1.0000e+00, 1.5682e-18, 1.6944e-17, 1.0227e-18]])\n",
      "tensor([[3.2192e-19, 1.0000e+00, 5.9989e-19, 2.0459e-18, 2.1829e-19],\n",
      "        [7.7345e-20, 1.0000e+00, 2.2071e-19, 1.7662e-17, 8.5427e-20]])\n",
      "tensor([[3.6436e-20, 1.0000e+00, 1.8333e-19, 1.6546e-18, 4.4362e-20],\n",
      "        [3.1210e-20, 1.0000e+00, 1.8403e-19, 3.4874e-18, 6.6291e-20]])\n",
      "tensor([[2.0119e-19, 1.0000e+00, 3.4455e-19, 6.7185e-17, 3.6336e-19],\n",
      "        [1.0353e-19, 1.0000e+00, 7.7051e-19, 1.1419e-17, 3.2964e-19]])\n",
      "tensor([[3.4432e-19, 1.0000e+00, 1.6702e-18, 1.3467e-16, 6.6426e-19],\n",
      "        [1.2702e-16, 1.0000e+00, 3.2320e-16, 2.1397e-16, 7.4500e-17]])\n",
      "tensor([[1.0827e-18, 1.0000e+00, 3.1063e-18, 4.0704e-17, 7.3935e-19],\n",
      "        [1.8508e-18, 1.0000e+00, 2.9685e-18, 1.7404e-16, 2.2025e-18]])\n",
      "tensor([[5.8758e-19, 1.0000e+00, 1.5515e-18, 1.3746e-16, 4.9350e-19],\n",
      "        [4.5152e-18, 1.0000e+00, 2.5229e-17, 4.9147e-18, 7.8095e-18]])\n",
      "tensor([[6.3732e-20, 1.0000e+00, 9.4565e-19, 2.9467e-18, 2.2487e-19],\n",
      "        [6.0468e-18, 1.0000e+00, 1.6211e-17, 4.4078e-16, 3.7587e-18]])\n",
      "tensor([[1.0164e-18, 1.0000e+00, 1.0790e-17, 2.1327e-16, 2.2502e-18],\n",
      "        [1.7854e-19, 1.0000e+00, 5.2627e-19, 6.2797e-18, 2.0078e-19]])\n",
      "tensor([[1.2345e-18, 1.0000e+00, 7.9362e-18, 1.8565e-16, 1.2017e-18],\n",
      "        [1.6623e-20, 1.0000e+00, 5.8891e-20, 1.0956e-18, 1.3574e-20]])\n",
      "tensor([[2.1972e-18, 1.0000e+00, 7.6248e-18, 4.0028e-18, 2.2971e-18],\n",
      "        [2.2236e-17, 1.0000e+00, 7.0750e-17, 6.7861e-17, 2.8332e-17]])\n",
      "tensor([[8.4871e-19, 1.0000e+00, 3.6451e-18, 2.2501e-18, 8.6091e-19],\n",
      "        [2.3721e-18, 1.0000e+00, 1.4248e-17, 8.8806e-17, 9.2118e-19]])\n",
      "tensor([[1.4843e-18, 1.0000e+00, 4.8341e-18, 4.2736e-17, 2.6158e-18],\n",
      "        [3.3046e-19, 1.0000e+00, 1.4060e-18, 9.2062e-18, 2.0482e-19]])\n",
      "tensor([[1.8871e-18, 1.0000e+00, 6.6087e-18, 5.6624e-17, 1.7472e-18],\n",
      "        [4.6189e-18, 1.0000e+00, 2.5059e-17, 1.4272e-16, 7.3789e-18]])\n",
      "tensor([[6.9839e-20, 1.0000e+00, 4.5553e-19, 2.4815e-18, 5.2676e-20],\n",
      "        [3.9043e-19, 1.0000e+00, 1.0284e-18, 8.1238e-18, 3.7805e-19]])\n",
      "tensor([[4.2306e-20, 1.0000e+00, 1.7280e-19, 6.6248e-19, 6.2418e-20],\n",
      "        [2.7290e-19, 1.0000e+00, 6.8280e-19, 5.4283e-18, 3.0517e-19]])\n",
      "tensor([[1.2361e-18, 1.0000e+00, 4.6612e-18, 4.2003e-18, 6.5328e-19],\n",
      "        [1.6407e-18, 1.0000e+00, 3.5421e-18, 2.5997e-16, 8.2636e-19]])\n",
      "tensor([[3.3212e-19, 1.0000e+00, 1.3451e-18, 6.2029e-18, 3.8893e-19],\n",
      "        [5.7995e-19, 1.0000e+00, 4.4398e-18, 9.6825e-18, 8.6080e-19]])\n",
      "tensor([[1.2958e-19, 1.0000e+00, 4.9233e-19, 2.6415e-18, 1.3228e-19],\n",
      "        [8.6002e-19, 1.0000e+00, 6.3949e-18, 3.0326e-17, 2.1159e-18]])\n",
      "tensor([[1.0803e-20, 1.0000e+00, 3.4019e-20, 2.9617e-18, 9.8506e-21],\n",
      "        [1.0664e-18, 1.0000e+00, 3.5500e-18, 6.6729e-18, 8.7817e-19]])\n",
      "tensor([[3.5132e-20, 1.0000e+00, 2.1310e-19, 2.4062e-17, 6.0753e-20],\n",
      "        [1.7015e-18, 1.0000e+00, 2.6738e-18, 4.8365e-17, 1.6975e-18]])\n",
      "tensor([[9.9825e-19, 1.0000e+00, 9.2765e-18, 1.6677e-16, 2.9043e-18],\n",
      "        [2.8484e-18, 1.0000e+00, 8.9987e-18, 4.1455e-17, 3.3258e-18]])\n",
      "tensor([[1.3533e-18, 1.0000e+00, 9.0684e-18, 3.7398e-17, 3.9563e-18],\n",
      "        [2.0939e-18, 1.0000e+00, 2.6478e-18, 7.1222e-18, 2.6892e-18]])\n",
      "tensor([[1.1305e-18, 1.0000e+00, 2.5231e-18, 1.2238e-17, 4.7898e-19],\n",
      "        [2.5433e-18, 1.0000e+00, 8.2837e-18, 1.7913e-17, 9.5616e-19]])\n",
      "tensor([[8.8383e-19, 1.0000e+00, 2.1285e-18, 1.0280e-16, 1.0145e-18],\n",
      "        [1.4928e-18, 1.0000e+00, 1.5974e-17, 1.1638e-17, 2.4263e-18]])\n",
      "tensor([[5.3144e-20, 1.0000e+00, 2.5731e-19, 2.6573e-17, 4.9032e-20],\n",
      "        [1.7036e-18, 1.0000e+00, 8.9006e-18, 1.0420e-16, 1.5978e-18]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1), array(1)]\n",
      "Test Accuracy:  0.133\n",
      "Test Weighted F1 score:  0.008\n",
      "EPOCHS: 7 | LR:0.01 | DROPOUT:0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:06<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.795                 | Train Accuracy:  0.307                 | Val Loss:  0.779                 | Val Accuracy:  0.433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:58<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.772                 | Train Accuracy:  0.453                 | Val Loss:  0.764                 | Val Accuracy:  0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:01<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.755                 | Train Accuracy:  0.499                 | Val Loss:  0.744                 | Val Accuracy:  0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:58<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.722                 | Train Accuracy:  0.591                 | Val Loss:  0.707                 | Val Accuracy:  0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.685                 | Train Accuracy:  0.658                 | Val Loss:  0.676                 | Val Accuracy:  0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:57<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.656                 | Train Accuracy:  0.768                 | Val Loss:  0.651                 | Val Accuracy:  0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:07<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.628                 | Train Accuracy:  0.889                 | Val Loss:  0.617                 | Val Accuracy:  0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:52<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.598                 | Train Accuracy:  0.939                 | Val Loss:  0.584                 | Val Accuracy:  0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:01<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.566                 | Train Accuracy:  0.969                 | Val Loss:  0.549                 | Val Accuracy:  0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:03<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.537                 | Train Accuracy:  0.983                 | Val Loss:  0.528                 | Val Accuracy:  0.983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:00<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.519                 | Train Accuracy:  0.983                 | Val Loss:  0.515                 | Val Accuracy:  0.983\n",
      "tensor([[0.0276, 0.0252, 0.0404, 0.8895, 0.0173],\n",
      "        [0.5839, 0.0599, 0.0707, 0.1786, 0.1068]])\n",
      "tensor([[0.0332, 0.0297, 0.0232, 0.8875, 0.0264],\n",
      "        [0.5377, 0.0586, 0.2150, 0.1058, 0.0829]])\n",
      "tensor([[0.0147, 0.0137, 0.0153, 0.0131, 0.9432],\n",
      "        [0.7312, 0.0917, 0.0857, 0.0422, 0.0492]])\n",
      "tensor([[0.3530, 0.4495, 0.0271, 0.1025, 0.0678],\n",
      "        [0.8376, 0.0344, 0.0492, 0.0318, 0.0470]])\n",
      "tensor([[0.0203, 0.0132, 0.0160, 0.0189, 0.9316],\n",
      "        [0.0293, 0.0170, 0.0335, 0.9021, 0.0181]])\n",
      "tensor([[0.0486, 0.0267, 0.0910, 0.8022, 0.0314],\n",
      "        [0.0218, 0.0177, 0.0335, 0.9109, 0.0161]])\n",
      "tensor([[0.3761, 0.0484, 0.3821, 0.1031, 0.0903],\n",
      "        [0.0583, 0.0207, 0.8154, 0.0603, 0.0454]])\n",
      "tensor([[0.0721, 0.0343, 0.7787, 0.0595, 0.0553],\n",
      "        [0.0539, 0.0232, 0.8280, 0.0498, 0.0451]])\n",
      "tensor([[0.8171, 0.0550, 0.0546, 0.0352, 0.0380],\n",
      "        [0.0601, 0.0411, 0.7951, 0.0462, 0.0575]])\n",
      "tensor([[0.0279, 0.0152, 0.0477, 0.8870, 0.0222],\n",
      "        [0.0704, 0.0310, 0.7942, 0.0539, 0.0505]])\n",
      "tensor([[0.0244, 0.9519, 0.0057, 0.0108, 0.0073],\n",
      "        [0.0200, 0.0166, 0.0154, 0.0194, 0.9286]])\n",
      "tensor([[0.0131, 0.0216, 0.0195, 0.0263, 0.9195],\n",
      "        [0.0239, 0.0209, 0.0152, 0.0236, 0.9164]])\n",
      "tensor([[0.0148, 0.0209, 0.0169, 0.0244, 0.9230],\n",
      "        [0.0503, 0.0195, 0.8428, 0.0578, 0.0296]])\n",
      "tensor([[0.0240, 0.0177, 0.0245, 0.0228, 0.9110],\n",
      "        [0.0194, 0.9516, 0.0073, 0.0120, 0.0097]])\n",
      "tensor([[0.0222, 0.0190, 0.0384, 0.9004, 0.0201],\n",
      "        [0.0174, 0.0170, 0.0171, 0.0262, 0.9224]])\n",
      "tensor([[0.5270, 0.0508, 0.2490, 0.1060, 0.0672],\n",
      "        [0.0261, 0.0249, 0.0432, 0.8761, 0.0297]])\n",
      "tensor([[0.0165, 0.0132, 0.0185, 0.0194, 0.9323],\n",
      "        [0.0482, 0.0321, 0.8198, 0.0569, 0.0431]])\n",
      "tensor([[0.6860, 0.0657, 0.1006, 0.0789, 0.0688],\n",
      "        [0.0498, 0.0265, 0.8463, 0.0443, 0.0331]])\n",
      "tensor([[0.0272, 0.0168, 0.0340, 0.9012, 0.0208],\n",
      "        [0.0182, 0.9571, 0.0052, 0.0116, 0.0080]])\n",
      "tensor([[0.0782, 0.0237, 0.7500, 0.1081, 0.0400],\n",
      "        [0.0128, 0.9617, 0.0052, 0.0090, 0.0114]])\n",
      "tensor([[0.0311, 0.0149, 0.0571, 0.8779, 0.0190],\n",
      "        [0.0256, 0.0245, 0.0334, 0.8918, 0.0247]])\n",
      "tensor([[0.0189, 0.0168, 0.0217, 0.0277, 0.9148],\n",
      "        [0.0639, 0.0320, 0.7913, 0.0576, 0.0553]])\n",
      "tensor([[0.0620, 0.0397, 0.7814, 0.0754, 0.0416],\n",
      "        [0.0201, 0.9506, 0.0074, 0.0114, 0.0105]])\n",
      "tensor([[0.5880, 0.0455, 0.1058, 0.1282, 0.1325],\n",
      "        [0.0325, 0.0225, 0.0557, 0.8630, 0.0263]])\n",
      "tensor([[0.8486, 0.0473, 0.0408, 0.0260, 0.0372],\n",
      "        [0.0233, 0.0184, 0.0368, 0.8998, 0.0216]])\n",
      "tensor([[0.0953, 0.0337, 0.7251, 0.1042, 0.0418],\n",
      "        [0.0280, 0.9272, 0.0132, 0.0131, 0.0185]])\n",
      "tensor([[0.0184, 0.0153, 0.0508, 0.8925, 0.0231],\n",
      "        [0.2030, 0.5570, 0.0438, 0.1022, 0.0940]])\n",
      "tensor([[0.6433, 0.0412, 0.1891, 0.0551, 0.0713],\n",
      "        [0.6345, 0.0852, 0.0534, 0.0676, 0.1593]])\n",
      "tensor([[0.7974, 0.0593, 0.0550, 0.0505, 0.0378],\n",
      "        [0.0797, 0.0264, 0.6880, 0.1479, 0.0580]])\n",
      "tensor([[0.0197, 0.0152, 0.0244, 0.9255, 0.0152],\n",
      "        [0.8054, 0.0313, 0.0639, 0.0397, 0.0596]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(2), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "Test Accuracy:  0.983\n",
      "Test Weighted F1 score:  0.967\n",
      "EPOCHS: 11 | LR:5e-07 | DROPOUT:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [16:17<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.860                 | Train Accuracy:  0.182                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [17:24<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [17:13<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [17:26<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [17:23<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [17:16<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [17:17<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n",
      "tensor([[4.7704e-20, 2.5735e-17, 1.1823e-15, 1.0000e+00, 4.3110e-20],\n",
      "        [8.0285e-16, 2.1332e-13, 1.1376e-17, 1.0000e+00, 1.1022e-14]])\n",
      "tensor([[4.8252e-19, 1.8340e-14, 1.9066e-18, 1.0000e+00, 2.6128e-16],\n",
      "        [6.6329e-18, 4.9347e-10, 3.3036e-17, 1.0000e+00, 3.4957e-15]])\n",
      "tensor([[9.7802e-16, 5.0769e-14, 6.4068e-18, 1.0000e+00, 7.0147e-16],\n",
      "        [4.4492e-18, 3.2354e-14, 5.8216e-12, 1.0000e+00, 1.0424e-13]])\n",
      "tensor([[3.7706e-20, 5.2209e-20, 1.5546e-19, 1.0000e+00, 6.2547e-19],\n",
      "        [1.4439e-16, 8.9965e-14, 2.7017e-14, 1.0000e+00, 1.0015e-10]])\n",
      "tensor([[9.6704e-18, 1.8231e-13, 1.5709e-15, 1.0000e+00, 4.2016e-16],\n",
      "        [6.7369e-14, 5.4555e-11, 1.3528e-14, 1.0000e+00, 2.8540e-11]])\n",
      "tensor([[6.8041e-18, 3.8033e-12, 3.2733e-18, 1.0000e+00, 3.3241e-17],\n",
      "        [1.0122e-19, 7.8296e-14, 6.5148e-18, 1.0000e+00, 2.5586e-16]])\n",
      "tensor([[6.1551e-16, 3.1246e-11, 3.3250e-15, 1.0000e+00, 2.4140e-15],\n",
      "        [1.4982e-13, 3.0575e-16, 1.2267e-14, 1.0000e+00, 8.0308e-14]])\n",
      "tensor([[2.4265e-15, 1.2154e-11, 7.5823e-11, 1.0000e+00, 1.7606e-16],\n",
      "        [2.7424e-16, 3.1097e-11, 3.0638e-14, 1.0000e+00, 1.2903e-13]])\n",
      "tensor([[1.7776e-15, 4.7433e-15, 6.7500e-14, 1.0000e+00, 1.6243e-15],\n",
      "        [1.8355e-18, 5.0726e-15, 2.1528e-19, 1.0000e+00, 4.9371e-17]])\n",
      "tensor([[7.0064e-15, 1.0148e-11, 8.0131e-13, 1.0000e+00, 3.9085e-16],\n",
      "        [1.0628e-19, 2.2988e-13, 9.0216e-15, 1.0000e+00, 1.6422e-21]])\n",
      "tensor([[5.0506e-14, 4.2053e-12, 3.7444e-11, 1.0000e+00, 1.1281e-10],\n",
      "        [5.3102e-14, 8.9469e-12, 2.9504e-13, 1.0000e+00, 1.4309e-13]])\n",
      "tensor([[2.2527e-16, 1.6627e-10, 4.1963e-14, 1.0000e+00, 2.1998e-14],\n",
      "        [1.6956e-17, 4.8167e-14, 8.7561e-16, 1.0000e+00, 5.5302e-16]])\n",
      "tensor([[1.2491e-15, 1.9936e-13, 3.1316e-14, 1.0000e+00, 2.3114e-15],\n",
      "        [5.9098e-19, 9.5576e-11, 5.9848e-15, 1.0000e+00, 1.2546e-13]])\n",
      "tensor([[2.8726e-18, 2.1527e-10, 1.7715e-15, 1.0000e+00, 9.0146e-17],\n",
      "        [4.0166e-14, 5.0667e-12, 1.4417e-13, 1.0000e+00, 7.3581e-14]])\n",
      "tensor([[6.4611e-18, 5.1901e-15, 5.1909e-15, 1.0000e+00, 1.4664e-17],\n",
      "        [1.2729e-18, 1.2250e-15, 9.1360e-17, 1.0000e+00, 8.5759e-15]])\n",
      "tensor([[5.8726e-15, 1.3479e-11, 3.7049e-14, 1.0000e+00, 9.2253e-12],\n",
      "        [2.2804e-14, 7.9153e-16, 2.7092e-17, 1.0000e+00, 1.7269e-17]])\n",
      "tensor([[9.0597e-20, 6.7532e-16, 1.3927e-15, 1.0000e+00, 2.8585e-16],\n",
      "        [2.9102e-18, 2.6773e-13, 4.7647e-15, 1.0000e+00, 1.9981e-17]])\n",
      "tensor([[8.3553e-18, 3.2768e-13, 2.9467e-17, 1.0000e+00, 4.8069e-20],\n",
      "        [2.2398e-19, 4.3069e-16, 1.5830e-16, 1.0000e+00, 2.2217e-22]])\n",
      "tensor([[7.5936e-19, 1.6240e-17, 2.9342e-19, 1.0000e+00, 2.4989e-16],\n",
      "        [2.1422e-15, 2.0207e-11, 8.9491e-13, 1.0000e+00, 8.6110e-13]])\n",
      "tensor([[6.2154e-15, 2.0274e-10, 4.5312e-13, 1.0000e+00, 3.1783e-14],\n",
      "        [6.3451e-16, 9.6815e-14, 4.0479e-16, 1.0000e+00, 2.6001e-16]])\n",
      "tensor([[2.0780e-19, 1.0028e-13, 5.9087e-22, 1.0000e+00, 8.4235e-16],\n",
      "        [1.2716e-20, 1.0772e-13, 1.3377e-16, 1.0000e+00, 5.9491e-19]])\n",
      "tensor([[4.3476e-20, 1.0886e-18, 7.2298e-19, 1.0000e+00, 1.5625e-19],\n",
      "        [8.4186e-19, 1.2174e-14, 2.5220e-18, 1.0000e+00, 7.5285e-15]])\n",
      "tensor([[8.7488e-17, 1.2086e-12, 1.0102e-13, 1.0000e+00, 4.1563e-15],\n",
      "        [4.8501e-20, 1.4520e-17, 1.2820e-16, 1.0000e+00, 1.0953e-20]])\n",
      "tensor([[8.8739e-16, 3.2445e-12, 5.0956e-15, 1.0000e+00, 1.0881e-15],\n",
      "        [4.0494e-19, 8.2871e-17, 1.9060e-16, 1.0000e+00, 1.6741e-21]])\n",
      "tensor([[2.1347e-20, 7.1671e-16, 5.1721e-18, 1.0000e+00, 3.3031e-18],\n",
      "        [8.2225e-13, 6.7171e-11, 8.2175e-14, 1.0000e+00, 5.7840e-12]])\n",
      "tensor([[1.4981e-18, 1.5219e-17, 3.7825e-22, 1.0000e+00, 4.2276e-20],\n",
      "        [1.2805e-12, 1.6614e-13, 1.5991e-08, 1.0000e+00, 2.9454e-12]])\n",
      "tensor([[3.2274e-19, 2.9087e-18, 1.0546e-18, 1.0000e+00, 1.1478e-18],\n",
      "        [2.0452e-17, 5.5137e-15, 4.6833e-16, 1.0000e+00, 4.9156e-14]])\n",
      "tensor([[9.4242e-19, 9.4573e-17, 5.2898e-18, 1.0000e+00, 9.4322e-18],\n",
      "        [7.8415e-15, 8.4863e-11, 3.4541e-14, 1.0000e+00, 2.8718e-13]])\n",
      "tensor([[3.9089e-15, 4.0694e-13, 1.7473e-12, 1.0000e+00, 9.6604e-16],\n",
      "        [1.8172e-18, 1.8447e-15, 1.6772e-19, 1.0000e+00, 1.0226e-18]])\n",
      "tensor([[2.0013e-18, 6.3431e-17, 6.2953e-16, 1.0000e+00, 4.8480e-17],\n",
      "        [3.3538e-16, 5.5383e-12, 1.0905e-14, 1.0000e+00, 4.5896e-16]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3)]\n",
      "Test Accuracy:  0.250\n",
      "Test Weighted F1 score:  0.138\n",
      "EPOCHS: 7 | LR:0.01 | DROPOUT:0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:07<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.816                 | Train Accuracy:  0.211                 | Val Loss:  0.818                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:05<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.821                 | Train Accuracy:  0.190                 | Val Loss:  0.830                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:00<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.824                 | Train Accuracy:  0.171                 | Val Loss:  0.826                 | Val Accuracy:  0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.824                 | Train Accuracy:  0.169                 | Val Loss:  0.809                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:03<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.814                 | Train Accuracy:  0.207                 | Val Loss:  0.823                 | Val Accuracy:  0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:01<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.822                 | Train Accuracy:  0.188                 | Val Loss:  0.835                 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:47<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.806                 | Train Accuracy:  0.238                 | Val Loss:  0.837                 | Val Accuracy:  0.133\n",
      "tensor([[0.2187, 0.1930, 0.0636, 0.1667, 0.3580],\n",
      "        [0.2436, 0.1997, 0.1217, 0.2562, 0.1788]])\n",
      "tensor([[0.4914, 0.0170, 0.0989, 0.0862, 0.3064],\n",
      "        [0.3724, 0.0352, 0.1190, 0.2025, 0.2709]])\n",
      "tensor([[0.0536, 0.2135, 0.5984, 0.1002, 0.0344],\n",
      "        [0.7738, 0.0220, 0.0134, 0.0239, 0.1669]])\n",
      "tensor([[0.2727, 0.1245, 0.4611, 0.0454, 0.0963],\n",
      "        [0.0803, 0.3621, 0.0528, 0.0154, 0.4893]])\n",
      "tensor([[0.5003, 0.0290, 0.0380, 0.3390, 0.0936],\n",
      "        [0.1654, 0.2095, 0.2956, 0.0707, 0.2588]])\n",
      "tensor([[0.4606, 0.0242, 0.4595, 0.0328, 0.0228],\n",
      "        [0.3584, 0.0476, 0.0307, 0.0656, 0.4977]])\n",
      "tensor([[0.0315, 0.0795, 0.5201, 0.3360, 0.0330],\n",
      "        [0.0062, 0.3324, 0.3857, 0.1600, 0.1156]])\n",
      "tensor([[0.2031, 0.2008, 0.1538, 0.1321, 0.3103],\n",
      "        [0.0738, 0.1053, 0.4369, 0.3146, 0.0695]])\n",
      "tensor([[0.1887, 0.1189, 0.1614, 0.4356, 0.0955],\n",
      "        [0.2825, 0.2010, 0.4028, 0.0555, 0.0583]])\n",
      "tensor([[0.1322, 0.0823, 0.0083, 0.6523, 0.1249],\n",
      "        [0.2288, 0.1104, 0.0474, 0.0069, 0.6065]])\n",
      "tensor([[0.2932, 0.4933, 0.1014, 0.0980, 0.0142],\n",
      "        [0.0953, 0.0598, 0.3238, 0.2880, 0.2331]])\n",
      "tensor([[0.3118, 0.3303, 0.1317, 0.0427, 0.1834],\n",
      "        [0.2029, 0.4713, 0.2600, 0.0370, 0.0289]])\n",
      "tensor([[0.5056, 0.4025, 0.0466, 0.0055, 0.0398],\n",
      "        [0.0483, 0.4435, 0.3284, 0.1100, 0.0698]])\n",
      "tensor([[0.0366, 0.0524, 0.1005, 0.6104, 0.2001],\n",
      "        [0.0540, 0.2206, 0.5642, 0.0394, 0.1217]])\n",
      "tensor([[0.4653, 0.1515, 0.1241, 0.1720, 0.0871],\n",
      "        [0.8426, 0.0143, 0.0245, 0.1095, 0.0091]])\n",
      "tensor([[0.2323, 0.0873, 0.5849, 0.0295, 0.0659],\n",
      "        [0.7206, 0.0802, 0.0246, 0.1563, 0.0183]])\n",
      "tensor([[0.0989, 0.1395, 0.7004, 0.0359, 0.0253],\n",
      "        [0.2564, 0.0164, 0.6127, 0.0667, 0.0478]])\n",
      "tensor([[0.0923, 0.0612, 0.1101, 0.0425, 0.6938],\n",
      "        [0.0841, 0.0910, 0.0640, 0.0854, 0.6755]])\n",
      "tensor([[0.1985, 0.0360, 0.4238, 0.1155, 0.2262],\n",
      "        [0.2146, 0.0028, 0.0109, 0.0124, 0.7591]])\n",
      "tensor([[0.3426, 0.2654, 0.0644, 0.0427, 0.2850],\n",
      "        [0.4928, 0.1408, 0.2229, 0.0943, 0.0493]])\n",
      "tensor([[0.2618, 0.0460, 0.2584, 0.2234, 0.2104],\n",
      "        [0.0435, 0.1994, 0.2283, 0.0313, 0.4975]])\n",
      "tensor([[0.0678, 0.0603, 0.0659, 0.5993, 0.2067],\n",
      "        [0.0891, 0.0236, 0.0375, 0.0359, 0.8139]])\n",
      "tensor([[0.1279, 0.1570, 0.1588, 0.0236, 0.5327],\n",
      "        [0.0556, 0.0016, 0.3941, 0.0316, 0.5171]])\n",
      "tensor([[0.1500, 0.2419, 0.1002, 0.0041, 0.5038],\n",
      "        [0.2079, 0.0956, 0.0721, 0.0462, 0.5782]])\n",
      "tensor([[0.0352, 0.0906, 0.1075, 0.1501, 0.6165],\n",
      "        [0.1785, 0.0009, 0.0119, 0.0168, 0.7920]])\n",
      "tensor([[0.6662, 0.1703, 0.0346, 0.0148, 0.1139],\n",
      "        [0.2062, 0.1316, 0.4874, 0.1034, 0.0713]])\n",
      "tensor([[0.1894, 0.1234, 0.3326, 0.1919, 0.1627],\n",
      "        [0.2438, 0.0320, 0.2751, 0.0104, 0.4387]])\n",
      "tensor([[0.7857, 0.0106, 0.0141, 0.0242, 0.1655],\n",
      "        [0.0713, 0.1869, 0.2405, 0.0151, 0.4861]])\n",
      "tensor([[0.1444, 0.6761, 0.1416, 0.0010, 0.0369],\n",
      "        [0.0087, 0.0201, 0.0448, 0.8082, 0.1181]])\n",
      "tensor([[0.2087, 0.1355, 0.2018, 0.3117, 0.1422],\n",
      "        [0.0884, 0.2723, 0.5136, 0.1034, 0.0222]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(4), array(0), array(2), array(2), array(0), array(0), array(2), array(4), array(3), array(3), array(1), array(1), array(0), array(3), array(0), array(2), array(2), array(4), array(2), array(0), array(0), array(3), array(4), array(4), array(4), array(0), array(2), array(0), array(1), array(3)]\n",
      "Test Accuracy:  0.167\n",
      "Test Weighted F1 score:  0.144\n",
      "EPOCHS: 7 | LR:5e-10 | DROPOUT:0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [19:44<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.847                 | Train Accuracy:  0.207                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:01<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.857                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:14<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:40<00:00,  5.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:36<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:19<00:00,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.857                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:06<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:09<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:01<00:00,  5.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:04<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.857                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:11<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.857                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:03<00:00,  5.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [23:24<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.858                 | Train Accuracy:  0.192                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n",
      "tensor([[7.3437e-22, 3.0270e-22, 1.0111e-21, 1.0000e+00, 1.9369e-23],\n",
      "        [6.6999e-25, 3.1314e-22, 8.3597e-21, 1.0000e+00, 8.4371e-27]])\n",
      "tensor([[1.2934e-23, 2.1171e-22, 1.9075e-21, 1.0000e+00, 5.3170e-28],\n",
      "        [1.1447e-25, 4.4598e-23, 1.6624e-22, 1.0000e+00, 9.5036e-27]])\n",
      "tensor([[9.9561e-25, 2.3512e-21, 1.4397e-19, 1.0000e+00, 4.1999e-26],\n",
      "        [1.1246e-26, 2.0924e-22, 2.5977e-25, 1.0000e+00, 2.4760e-28]])\n",
      "tensor([[6.8251e-22, 5.6684e-23, 2.9593e-20, 1.0000e+00, 2.7104e-27],\n",
      "        [3.6953e-25, 3.7912e-23, 6.7774e-23, 1.0000e+00, 3.2748e-29]])\n",
      "tensor([[1.8421e-23, 1.8001e-22, 3.0918e-23, 1.0000e+00, 1.2416e-27],\n",
      "        [7.6346e-26, 5.9249e-24, 1.1198e-26, 1.0000e+00, 1.9618e-29]])\n",
      "tensor([[4.7924e-20, 2.0106e-18, 3.6903e-19, 1.0000e+00, 2.0324e-25],\n",
      "        [7.9767e-26, 2.0750e-24, 1.3064e-24, 1.0000e+00, 7.0498e-30]])\n",
      "tensor([[3.7646e-25, 3.3690e-24, 1.4132e-21, 1.0000e+00, 2.4322e-27],\n",
      "        [1.0401e-22, 1.2874e-21, 5.7993e-20, 1.0000e+00, 8.4236e-28]])\n",
      "tensor([[1.6113e-27, 4.1917e-25, 1.1804e-25, 1.0000e+00, 6.8901e-30],\n",
      "        [1.4939e-25, 1.2856e-22, 1.7916e-24, 1.0000e+00, 1.4531e-27]])\n",
      "tensor([[4.8963e-27, 2.3903e-22, 4.0143e-21, 1.0000e+00, 6.0075e-27],\n",
      "        [6.1789e-25, 2.7011e-22, 6.3594e-20, 1.0000e+00, 6.9920e-28]])\n",
      "tensor([[5.1788e-26, 1.3341e-22, 1.2655e-21, 1.0000e+00, 4.4429e-27],\n",
      "        [1.7083e-22, 2.3369e-20, 2.9480e-21, 1.0000e+00, 5.5383e-26]])\n",
      "tensor([[7.5691e-26, 4.5946e-20, 8.5646e-25, 1.0000e+00, 8.9995e-27],\n",
      "        [4.5918e-25, 2.9841e-21, 4.9563e-23, 1.0000e+00, 6.7753e-29]])\n",
      "tensor([[7.1782e-24, 2.4350e-24, 1.4830e-22, 1.0000e+00, 1.5565e-26],\n",
      "        [8.4878e-25, 4.9314e-23, 7.7439e-24, 1.0000e+00, 5.6187e-28]])\n",
      "tensor([[1.7978e-27, 9.0859e-26, 2.4413e-24, 1.0000e+00, 1.1945e-28],\n",
      "        [3.6133e-25, 4.6800e-22, 1.7738e-21, 1.0000e+00, 5.1120e-30]])\n",
      "tensor([[7.0173e-26, 9.5694e-25, 6.8862e-23, 1.0000e+00, 8.9238e-29],\n",
      "        [5.8786e-23, 9.1100e-26, 3.3844e-22, 1.0000e+00, 5.0674e-27]])\n",
      "tensor([[8.7330e-25, 2.6416e-22, 6.6378e-23, 1.0000e+00, 6.5963e-28],\n",
      "        [6.5380e-27, 2.8748e-22, 1.4641e-23, 1.0000e+00, 5.2576e-28]])\n",
      "tensor([[3.6038e-22, 5.1026e-19, 2.0395e-18, 1.0000e+00, 6.9826e-23],\n",
      "        [1.5656e-22, 1.2153e-20, 2.3632e-22, 1.0000e+00, 9.1494e-29]])\n",
      "tensor([[1.0917e-28, 6.3799e-23, 1.8267e-26, 1.0000e+00, 1.3864e-30],\n",
      "        [6.6602e-27, 4.4930e-26, 6.1769e-29, 1.0000e+00, 1.5605e-29]])\n",
      "tensor([[4.6107e-26, 1.2963e-25, 6.5020e-27, 1.0000e+00, 7.1006e-30],\n",
      "        [1.1805e-26, 1.9505e-23, 1.9066e-22, 1.0000e+00, 4.7006e-27]])\n",
      "tensor([[3.4122e-27, 5.3559e-26, 1.7358e-24, 1.0000e+00, 2.7161e-30],\n",
      "        [1.0462e-21, 2.0590e-19, 1.4809e-21, 1.0000e+00, 2.0364e-26]])\n",
      "tensor([[7.5965e-25, 6.2657e-24, 4.4652e-23, 1.0000e+00, 1.7240e-27],\n",
      "        [4.0021e-26, 8.5122e-25, 4.6907e-23, 1.0000e+00, 1.1308e-29]])\n",
      "tensor([[1.4479e-24, 3.8983e-20, 2.6488e-17, 1.0000e+00, 6.3018e-27],\n",
      "        [5.6597e-21, 4.4263e-20, 2.5091e-18, 1.0000e+00, 5.4894e-25]])\n",
      "tensor([[5.8132e-25, 5.1312e-24, 5.5137e-23, 1.0000e+00, 5.4682e-28],\n",
      "        [6.9951e-22, 1.3915e-21, 2.1006e-23, 1.0000e+00, 1.1448e-27]])\n",
      "tensor([[1.0756e-21, 3.0726e-23, 2.7044e-23, 1.0000e+00, 1.9276e-25],\n",
      "        [9.3262e-25, 5.0213e-24, 7.7668e-24, 1.0000e+00, 3.6969e-31]])\n",
      "tensor([[1.8690e-24, 6.2236e-25, 4.7938e-20, 1.0000e+00, 7.7319e-27],\n",
      "        [6.7950e-25, 9.6737e-22, 5.6630e-23, 1.0000e+00, 2.2287e-28]])\n",
      "tensor([[9.2509e-23, 2.0118e-21, 2.5846e-19, 1.0000e+00, 3.0169e-27],\n",
      "        [6.0603e-29, 9.7728e-25, 4.3159e-27, 1.0000e+00, 7.2876e-31]])\n",
      "tensor([[2.2555e-29, 4.3864e-27, 5.0465e-26, 1.0000e+00, 1.1694e-31],\n",
      "        [6.1735e-24, 4.6640e-19, 8.4877e-23, 1.0000e+00, 3.2643e-26]])\n",
      "tensor([[3.8627e-25, 7.1265e-22, 7.1615e-23, 1.0000e+00, 2.3481e-28],\n",
      "        [3.9193e-27, 9.1728e-27, 3.3835e-26, 1.0000e+00, 1.2351e-30]])\n",
      "tensor([[1.0690e-22, 5.4361e-25, 1.9500e-22, 1.0000e+00, 2.5754e-30],\n",
      "        [4.5134e-24, 1.2015e-19, 1.8413e-21, 1.0000e+00, 1.2496e-26]])\n",
      "tensor([[1.9606e-25, 3.2640e-24, 2.9356e-22, 1.0000e+00, 4.0222e-29],\n",
      "        [4.0864e-27, 2.7409e-25, 3.7601e-22, 1.0000e+00, 1.6578e-26]])\n",
      "tensor([[3.7508e-28, 4.9004e-22, 2.4714e-21, 1.0000e+00, 1.4480e-30],\n",
      "        [1.3675e-25, 2.5426e-24, 3.4895e-24, 1.0000e+00, 3.2484e-28]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3), array(3)]\n",
      "Test Accuracy:  0.250\n",
      "Test Weighted F1 score:  0.138\n",
      "EPOCHS: 13 | LR:0.01 | DROPOUT:0.7000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.813                 | Train Accuracy:  0.188                 | Val Loss:  0.811                 | Val Accuracy:  0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [08:59<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.816                 | Train Accuracy:  0.150                 | Val Loss:  0.801                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:03<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.819                 | Train Accuracy:  0.167                 | Val Loss:  0.801                 | Val Accuracy:  0.217\n",
      "tensor([[0.1119, 0.0738, 0.1306, 0.6183, 0.0654],\n",
      "        [0.0764, 0.1848, 0.0465, 0.4048, 0.2876]])\n",
      "tensor([[0.0903, 0.1332, 0.0682, 0.4714, 0.2369],\n",
      "        [0.0866, 0.1136, 0.1168, 0.4558, 0.2273]])\n",
      "tensor([[0.1956, 0.0935, 0.0914, 0.5710, 0.0485],\n",
      "        [0.1315, 0.1693, 0.0765, 0.3887, 0.2340]])\n",
      "tensor([[0.2095, 0.4038, 0.1322, 0.0923, 0.1622],\n",
      "        [0.1634, 0.0281, 0.4362, 0.3308, 0.0414]])\n",
      "tensor([[0.1651, 0.0328, 0.0423, 0.5165, 0.2434],\n",
      "        [0.1435, 0.1156, 0.1027, 0.6008, 0.0374]])\n",
      "tensor([[0.0939, 0.1322, 0.2534, 0.1885, 0.3319],\n",
      "        [0.0940, 0.2821, 0.1442, 0.3120, 0.1676]])\n",
      "tensor([[0.2250, 0.1379, 0.1504, 0.2885, 0.1982],\n",
      "        [0.2740, 0.1692, 0.0690, 0.2317, 0.2561]])\n",
      "tensor([[0.2685, 0.1982, 0.1450, 0.1888, 0.1995],\n",
      "        [0.2212, 0.1566, 0.0527, 0.1363, 0.4332]])\n",
      "tensor([[0.2181, 0.1227, 0.1796, 0.3141, 0.1655],\n",
      "        [0.2458, 0.0666, 0.1239, 0.3228, 0.2409]])\n",
      "tensor([[0.0551, 0.0454, 0.0643, 0.5890, 0.2462],\n",
      "        [0.0909, 0.2154, 0.0332, 0.5807, 0.0798]])\n",
      "tensor([[0.1998, 0.1065, 0.2234, 0.2001, 0.2702],\n",
      "        [0.2818, 0.1494, 0.0367, 0.2258, 0.3062]])\n",
      "tensor([[0.0474, 0.0314, 0.0680, 0.7612, 0.0920],\n",
      "        [0.1143, 0.3062, 0.0776, 0.3834, 0.1184]])\n",
      "tensor([[0.2182, 0.2171, 0.1603, 0.3543, 0.0501],\n",
      "        [0.3717, 0.1824, 0.1075, 0.1920, 0.1464]])\n",
      "tensor([[0.2778, 0.1539, 0.1612, 0.2933, 0.1138],\n",
      "        [0.2306, 0.0896, 0.2050, 0.3722, 0.1026]])\n",
      "tensor([[0.1491, 0.2429, 0.1074, 0.1823, 0.3184],\n",
      "        [0.1077, 0.1561, 0.3713, 0.1281, 0.2368]])\n",
      "tensor([[0.3175, 0.1409, 0.1262, 0.1990, 0.2164],\n",
      "        [0.1617, 0.0257, 0.0929, 0.4530, 0.2667]])\n",
      "tensor([[0.3551, 0.1978, 0.2651, 0.0331, 0.1489],\n",
      "        [0.0653, 0.1297, 0.2400, 0.1113, 0.4537]])\n",
      "tensor([[0.3014, 0.3605, 0.0675, 0.2014, 0.0691],\n",
      "        [0.0295, 0.2293, 0.2305, 0.1413, 0.3695]])\n",
      "tensor([[0.3187, 0.1298, 0.2909, 0.2026, 0.0579],\n",
      "        [0.1043, 0.0602, 0.1005, 0.5955, 0.1396]])\n",
      "tensor([[0.1771, 0.3528, 0.0491, 0.2893, 0.1316],\n",
      "        [0.0192, 0.0724, 0.0531, 0.7296, 0.1257]])\n",
      "tensor([[0.0374, 0.0427, 0.0533, 0.1750, 0.6917],\n",
      "        [0.1083, 0.0859, 0.1839, 0.5371, 0.0848]])\n",
      "tensor([[0.0443, 0.1839, 0.1930, 0.2431, 0.3356],\n",
      "        [0.0946, 0.1420, 0.0946, 0.2953, 0.3736]])\n",
      "tensor([[0.4150, 0.0455, 0.1584, 0.2321, 0.1490],\n",
      "        [0.2126, 0.1303, 0.0450, 0.5368, 0.0753]])\n",
      "tensor([[0.0515, 0.1725, 0.2602, 0.1263, 0.3895],\n",
      "        [0.2642, 0.0649, 0.1172, 0.1670, 0.3866]])\n",
      "tensor([[0.3442, 0.0907, 0.2119, 0.2879, 0.0653],\n",
      "        [0.1562, 0.1735, 0.2397, 0.3261, 0.1045]])\n",
      "tensor([[0.2397, 0.0749, 0.1503, 0.2118, 0.3233],\n",
      "        [0.2124, 0.1001, 0.0835, 0.1239, 0.4801]])\n",
      "tensor([[0.3672, 0.1083, 0.1065, 0.2019, 0.2161],\n",
      "        [0.0878, 0.2489, 0.0956, 0.3849, 0.1828]])\n",
      "tensor([[0.0785, 0.1484, 0.0691, 0.3165, 0.3875],\n",
      "        [0.6229, 0.0829, 0.0664, 0.1412, 0.0866]])\n",
      "tensor([[0.1161, 0.1197, 0.2601, 0.1802, 0.3238],\n",
      "        [0.0902, 0.2590, 0.1210, 0.4179, 0.1119]])\n",
      "tensor([[0.0986, 0.1651, 0.4075, 0.2828, 0.0459],\n",
      "        [0.3229, 0.1883, 0.1422, 0.2388, 0.1077]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(3), array(3), array(3), array(1), array(3), array(4), array(3), array(0), array(3), array(3), array(4), array(3), array(3), array(3), array(4), array(0), array(0), array(1), array(0), array(1), array(4), array(4), array(0), array(4), array(0), array(4), array(0), array(4), array(4), array(2)]\n",
      "Test Accuracy:  0.250\n",
      "Test Weighted F1 score:  0.222\n",
      "EPOCHS: 3 | LR:5e-11 | DROPOUT:0.7000000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:08<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.836                 | Train Accuracy:  0.173                 | Val Loss:  0.819                 | Val Accuracy:  0.233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:08<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.837                 | Train Accuracy:  0.203                 | Val Loss:  0.852                 | Val Accuracy:  0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:07<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.853                 | Train Accuracy:  0.192                 | Val Loss:  0.853                 | Val Accuracy:  0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:07<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.842                 | Train Accuracy:  0.186                 | Val Loss:  0.860                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:02<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.851                 | Train Accuracy:  0.198                 | Val Loss:  0.847                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.839                 | Train Accuracy:  0.203                 | Val Loss:  0.859                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:13<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.850                 | Train Accuracy:  0.205                 | Val Loss:  0.855                 | Val Accuracy:  0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:10<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.840                 | Train Accuracy:  0.200                 | Val Loss:  0.848                 | Val Accuracy:  0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:09<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.842                 | Train Accuracy:  0.196                 | Val Loss:  0.828                 | Val Accuracy:  0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:07<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.841                 | Train Accuracy:  0.200                 | Val Loss:  0.844                 | Val Accuracy:  0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [09:13<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.853                 | Train Accuracy:  0.186                 | Val Loss:  0.870                 | Val Accuracy:  0.150\n",
      "tensor([[7.1878e-06, 1.6004e-03, 9.9787e-01, 4.4752e-05, 4.7538e-04],\n",
      "        [3.0621e-02, 5.0514e-03, 9.4801e-01, 5.3109e-04, 1.5787e-02]])\n",
      "tensor([[2.4235e-06, 7.2856e-05, 9.9992e-01, 4.2740e-06, 1.5519e-07],\n",
      "        [2.3811e-08, 2.6640e-06, 9.9998e-01, 6.3836e-07, 1.2675e-05]])\n",
      "tensor([[2.4035e-02, 6.7508e-03, 7.7814e-01, 3.0694e-04, 1.9076e-01],\n",
      "        [6.3461e-01, 2.3774e-03, 3.5966e-01, 3.0598e-04, 3.0455e-03]])\n",
      "tensor([[3.4390e-02, 1.8507e-03, 7.5988e-01, 9.4811e-05, 2.0379e-01],\n",
      "        [2.3683e-05, 1.6324e-08, 9.9974e-01, 8.3355e-07, 2.3476e-04]])\n",
      "tensor([[7.8527e-03, 1.2255e-05, 9.8942e-01, 2.1016e-05, 2.6971e-03],\n",
      "        [4.8760e-04, 1.1843e-03, 9.9787e-01, 4.4642e-04, 9.6985e-06]])\n",
      "tensor([[6.0508e-04, 6.4222e-03, 9.6458e-01, 1.5527e-03, 2.6837e-02],\n",
      "        [6.6404e-09, 1.8869e-06, 9.9975e-01, 9.0409e-11, 2.4756e-04]])\n",
      "tensor([[2.6262e-04, 3.2725e-04, 8.0982e-01, 1.5165e-06, 1.8958e-01],\n",
      "        [7.8130e-11, 1.5406e-07, 1.0000e+00, 5.3641e-09, 6.7717e-09]])\n",
      "tensor([[4.2383e-02, 7.8382e-04, 9.0319e-01, 6.5752e-04, 5.2986e-02],\n",
      "        [3.7001e-03, 7.0461e-05, 9.9614e-01, 1.8504e-05, 7.1897e-05]])\n",
      "tensor([[3.5676e-11, 9.7098e-09, 9.9992e-01, 1.3953e-09, 7.8985e-05],\n",
      "        [1.6317e-03, 2.2499e-04, 9.9812e-01, 1.7120e-05, 2.0353e-06]])\n",
      "tensor([[4.2152e-04, 5.1706e-06, 9.9838e-01, 4.4112e-08, 1.1965e-03],\n",
      "        [6.9301e-04, 1.2739e-04, 9.9916e-01, 7.8081e-07, 1.5072e-05]])\n",
      "tensor([[1.7244e-03, 2.5261e-04, 9.9729e-01, 5.4399e-05, 6.7803e-04],\n",
      "        [1.6776e-06, 1.6411e-05, 9.9996e-01, 7.3572e-07, 2.4177e-05]])\n",
      "tensor([[1.2229e-04, 1.3991e-04, 9.9272e-01, 1.0371e-04, 6.9150e-03],\n",
      "        [6.8456e-05, 4.0988e-06, 9.9949e-01, 1.2080e-06, 4.3922e-04]])\n",
      "tensor([[4.3439e-06, 3.7791e-03, 9.9621e-01, 7.4653e-07, 2.7221e-06],\n",
      "        [2.2712e-05, 5.9875e-03, 9.9397e-01, 3.3002e-06, 1.7063e-05]])\n",
      "tensor([[2.2538e-05, 3.1602e-04, 9.8981e-01, 5.2164e-07, 9.8514e-03],\n",
      "        [6.8690e-02, 3.3799e-06, 9.2519e-01, 1.5770e-06, 6.1155e-03]])\n",
      "tensor([[1.6786e-04, 4.8689e-06, 9.9983e-01, 5.0596e-07, 3.9880e-07],\n",
      "        [9.0001e-04, 1.1246e-04, 9.9895e-01, 4.2057e-05, 1.9280e-07]])\n",
      "tensor([[2.7260e-02, 1.1638e-04, 9.7248e-01, 1.5949e-05, 1.2493e-04],\n",
      "        [1.1142e-04, 4.3599e-06, 9.5320e-01, 3.7514e-05, 4.6642e-02]])\n",
      "tensor([[1.2182e-05, 9.7965e-09, 9.9998e-01, 1.8584e-07, 4.3559e-06],\n",
      "        [5.8994e-05, 3.8827e-04, 9.9954e-01, 7.1555e-06, 7.2563e-06]])\n",
      "tensor([[1.7677e-04, 8.6815e-05, 9.9717e-01, 2.5476e-06, 2.5609e-03],\n",
      "        [4.5834e-01, 6.1012e-04, 5.3101e-01, 1.2757e-04, 9.9191e-03]])\n",
      "tensor([[1.9533e-03, 1.0563e-05, 9.9785e-01, 1.0640e-07, 1.8313e-04],\n",
      "        [2.2898e-05, 1.3202e-05, 9.9885e-01, 8.2024e-08, 1.1103e-03]])\n",
      "tensor([[2.5739e-06, 1.4698e-04, 9.9953e-01, 1.1239e-08, 3.2283e-04],\n",
      "        [1.7602e-05, 3.2596e-05, 9.9989e-01, 4.4156e-05, 1.5641e-05]])\n",
      "tensor([[3.3622e-03, 3.7121e-05, 9.8544e-01, 8.1463e-06, 1.1153e-02],\n",
      "        [1.0458e-03, 2.1840e-05, 9.9865e-01, 2.7499e-04, 5.2055e-06]])\n",
      "tensor([[8.9147e-05, 8.7252e-06, 9.8814e-01, 1.6909e-07, 1.1758e-02],\n",
      "        [2.2847e-10, 1.6244e-07, 1.0000e+00, 2.7767e-09, 2.8678e-07]])\n",
      "tensor([[2.5830e-03, 1.3918e-05, 9.9730e-01, 6.7140e-06, 9.5113e-05],\n",
      "        [1.2835e-02, 6.6128e-04, 9.8440e-01, 9.5216e-05, 2.0093e-03]])\n",
      "tensor([[1.6337e-10, 1.6485e-08, 9.9979e-01, 9.3225e-09, 2.1256e-04],\n",
      "        [9.0634e-06, 2.0420e-03, 9.9405e-01, 7.3465e-05, 3.8266e-03]])\n",
      "tensor([[2.0362e-01, 1.5092e-04, 7.9514e-01, 6.3977e-04, 4.5555e-04],\n",
      "        [2.6441e-05, 4.0336e-07, 9.9995e-01, 2.0143e-07, 2.2977e-05]])\n",
      "tensor([[3.6013e-05, 4.0526e-05, 9.9991e-01, 1.0708e-06, 1.2781e-05],\n",
      "        [1.9906e-05, 1.4256e-06, 9.9946e-01, 1.6872e-08, 5.1800e-04]])\n",
      "tensor([[4.1597e-04, 1.9686e-05, 9.9930e-01, 2.9951e-05, 2.3657e-04],\n",
      "        [5.6157e-05, 1.5912e-06, 9.9986e-01, 6.5381e-07, 7.9044e-05]])\n",
      "tensor([[9.9710e-08, 7.9347e-06, 9.9997e-01, 7.9011e-10, 1.8823e-05],\n",
      "        [5.6601e-06, 2.3436e-08, 9.9980e-01, 1.9520e-06, 1.9294e-04]])\n",
      "tensor([[2.9769e-03, 1.7752e-04, 9.4728e-01, 1.8216e-04, 4.9384e-02],\n",
      "        [3.7086e-02, 9.0028e-04, 9.6175e-01, 1.7963e-04, 8.7254e-05]])\n",
      "tensor([[2.7047e-05, 7.0965e-05, 9.9981e-01, 2.1368e-05, 7.1355e-05],\n",
      "        [1.9827e-04, 3.9229e-06, 9.9964e-01, 1.2292e-04, 3.2365e-05]])\n",
      "[array(3), array(3), array(4), array(1), array(4), array(3), array(0), array(2), array(0), array(3), array(1), array(4), array(4), array(4), array(3), array(0), array(4), array(0), array(3), array(2), array(3), array(4), array(2), array(0), array(0), array(2), array(3), array(0), array(0), array(3)]\n",
      "[array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2), array(2)]\n",
      "Test Accuracy:  0.233\n",
      "Test Weighted F1 score:  0.031\n",
      "EPOCHS: 11 | LR:0.001 | DROPOUT:0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/240 [00:00<?, ?it/s]/tmp/ipykernel_155/1648410969.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  final_layer = self.relu(linear_output)\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 240/240 [10:24<00:00,  2.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m DROPOUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m9\u001b[39m)\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m BertClassifier(dropout\u001b[38;5;241m=\u001b[39mDROPOUT)           \n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m acc, f1 \u001b[38;5;241m=\u001b[39m evaluate(model, df_test)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | LR:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | DROPOUT:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDROPOUT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     56\u001b[0m mask \u001b[38;5;241m=\u001b[39m val_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m input_id \u001b[38;5;241m=\u001b[39m val_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 59\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(output, val_label)\n\u001b[1;32m     62\u001b[0m total_loss_val \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mBertClassifier.forward\u001b[0;34m(self, input_id, mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_id, mask):\n\u001b[0;32m---> 19\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     dropout_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n\u001b[1;32m     21\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dropout_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:999\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    990\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    992\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    993\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    994\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    998\u001b[0m )\n\u001b[0;32m--> 999\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1012\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    510\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    511\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 513\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/transformers/modeling_utils.py:2361\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2358\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m-> 2361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    525\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 526\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:439\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 439\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    441\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##split\n",
    "import random\n",
    "from csv import writer\n",
    "\n",
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "print(len(df_train),len(df_val), len(df_test))\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 1e-6\n",
    "\n",
    "for i in range(200):\n",
    "    EPOCHS = random.randint(2,15)\n",
    "    LR = 0.5 * random.randint(1,2) * 10 ** (-random.randint(1,10))\n",
    "    DROPOUT = 0.1 * random.randint(1,9)\n",
    "    model = BertClassifier(dropout=DROPOUT)           \n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    acc, f1 = evaluate(model, df_test)\n",
    "    print(f'EPOCHS: {EPOCHS} | LR:{LR} | DROPOUT:{DROPOUT}')\n",
    "    list_data = [EPOCHS,LR,DROPOUT,acc,f1]\n",
    "    with open('data/training_category.csv','a', newline='') as f_object:\n",
    "        writer_object = writer(f_object)\n",
    "        writer_object.writerow(list_data)\n",
    "        f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b8464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
